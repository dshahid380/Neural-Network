{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "def read_dataset():\n",
    "    df=pd.read_csv('train.csv')\n",
    "    X=df[df.columns[1:5]].values\n",
    "    y=df[df.columns[5]]\n",
    "    \n",
    "    #Encode the dependent variable\n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    Y=one_hot_encode(y)\n",
    "    print(X.shape)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the encoder function\n",
    "def one_hot_encode(labels):\n",
    "    n_labels=len(labels)\n",
    "    n_unique_labels=len(np.unique(labels))\n",
    "    one_hot_encode=np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels),labels]=1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1060, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y=read_dataset()\n",
    "X,Y=shuffle(X,Y,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848, 4)\n",
      "(848, 2)\n",
      "(212, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.20)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "epochs=1000\n",
    "cost_history=np.empty(shape=[1],dtype=float)\n",
    "n_dim=X.shape[1]\n",
    "n_class=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1=10\n",
    "n_hidden_2=10\n",
    "n_hidden_3=10\n",
    "n_hidden_4=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,n_dim])\n",
    "w=tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "b=tf.Variable(tf.zeros([n_dim]))\n",
    "y_=tf.placeholder(tf.float32,[None,n_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights ={\n",
    "          'h1':tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
    "          'h2':tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "          'h3':tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "          'h4':tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "         'out':tf.Variable(tf.truncated_normal([n_hidden_4,n_class]))\n",
    "        }\n",
    "biases ={\n",
    "          'b1':tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "          'b2':tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "          'b3':tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "          'b4':tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "         'out':tf.Variable(tf.truncated_normal([n_class]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x,weights,biases):\n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "    layer_3=tf.add(tf.matmul(layer_2,weights['h3']),biases['b3'])\n",
    "    layer_3=tf.nn.sigmoid(layer_3)\n",
    "    \n",
    "    layer_4=tf.add(tf.matmul(layer_3,weights['h4']),biases['b4'])\n",
    "    layer_4=tf.nn.relu(layer_4)\n",
    "    \n",
    "    out_layer=tf.matmul(layer_4,weights['out'])+biases['out']\n",
    "    return out_layer\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-b4e11b0c2609>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=neural_network(x,weights,biases)\n",
    "\n",
    "cost_function=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))\n",
    "training_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "sess=tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  -  cost : 1.1478521  -MSE  5.144580015782134  -Training accuracy : 0.4186321\n",
      "epoch:  1  -  cost : 1.0992295  -MSE  5.207371229941786  -Training accuracy : 0.4186321\n",
      "epoch:  2  -  cost : 1.055501  -MSE  5.277241972175087  -Training accuracy : 0.4186321\n",
      "epoch:  3  -  cost : 1.0164015  -MSE  5.352438339687778  -Training accuracy : 0.4186321\n",
      "epoch:  4  -  cost : 0.981586  -MSE  5.431370447994842  -Training accuracy : 0.4186321\n",
      "epoch:  5  -  cost : 0.9507266  -MSE  5.512638441715183  -Training accuracy : 0.4186321\n",
      "epoch:  6  -  cost : 0.9234993  -MSE  5.5949815078224  -Training accuracy : 0.4186321\n",
      "epoch:  7  -  cost : 0.8997221  -MSE  5.677462458723961  -Training accuracy : 0.4186321\n",
      "epoch:  8  -  cost : 0.87894064  -MSE  5.759208412197315  -Training accuracy : 0.40801886\n",
      "epoch:  9  -  cost : 0.8608052  -MSE  5.839499900791872  -Training accuracy : 0.38915095\n",
      "epoch:  10  -  cost : 0.84501296  -MSE  5.917757783153597  -Training accuracy : 0.3667453\n",
      "epoch:  11  -  cost : 0.8312604  -MSE  5.993496805341351  -Training accuracy : 0.34551886\n",
      "epoch:  12  -  cost : 0.8193172  -MSE  6.066447512832185  -Training accuracy : 0.3231132\n",
      "epoch:  13  -  cost : 0.8089011  -MSE  6.136433670896918  -Training accuracy : 0.30070755\n",
      "epoch:  14  -  cost : 0.7998152  -MSE  6.2032723254162265  -Training accuracy : 0.2735849\n",
      "epoch:  15  -  cost : 0.7918551  -MSE  6.266923831574051  -Training accuracy : 0.2476415\n",
      "epoch:  16  -  cost : 0.78489524  -MSE  6.327368162151853  -Training accuracy : 0.2240566\n",
      "epoch:  17  -  cost : 0.77878964  -MSE  6.384615822932345  -Training accuracy : 0.20636792\n",
      "epoch:  18  -  cost : 0.77339894  -MSE  6.438749254579845  -Training accuracy : 0.22759435\n",
      "epoch:  19  -  cost : 0.7686099  -MSE  6.48977087127622  -Training accuracy : 0.22995283\n",
      "epoch:  20  -  cost : 0.76433176  -MSE  6.537857584082477  -Training accuracy : 0.2523585\n",
      "epoch:  21  -  cost : 0.7605338  -MSE  6.583024015654537  -Training accuracy : 0.27240565\n",
      "epoch:  22  -  cost : 0.75715476  -MSE  6.625439650450936  -Training accuracy : 0.2912736\n",
      "epoch:  23  -  cost : 0.7541503  -MSE  6.665216354371536  -Training accuracy : 0.30306605\n",
      "epoch:  24  -  cost : 0.75144434  -MSE  6.702504522194967  -Training accuracy : 0.317217\n",
      "epoch:  25  -  cost : 0.7490189  -MSE  6.737429990395905  -Training accuracy : 0.33254716\n",
      "epoch:  26  -  cost : 0.74683285  -MSE  6.770213741404483  -Training accuracy : 0.34551886\n",
      "epoch:  27  -  cost : 0.74485004  -MSE  6.8009253644972025  -Training accuracy : 0.39504716\n",
      "epoch:  28  -  cost : 0.7430413  -MSE  6.829713972222356  -Training accuracy : 0.43867925\n",
      "epoch:  29  -  cost : 0.74138945  -MSE  6.8566237666797125  -Training accuracy : 0.46226415\n",
      "epoch:  30  -  cost : 0.739871  -MSE  6.881787167645578  -Training accuracy : 0.4870283\n",
      "epoch:  31  -  cost : 0.73847675  -MSE  6.905348217115299  -Training accuracy : 0.5082547\n",
      "epoch:  32  -  cost : 0.7371711  -MSE  6.927440360688486  -Training accuracy : 0.5212264\n",
      "epoch:  33  -  cost : 0.73595214  -MSE  6.948174768894071  -Training accuracy : 0.5365566\n",
      "epoch:  34  -  cost : 0.7347999  -MSE  6.967578065350425  -Training accuracy : 0.5495283\n",
      "epoch:  35  -  cost : 0.73370147  -MSE  6.985877466955312  -Training accuracy : 0.5554245\n",
      "epoch:  36  -  cost : 0.73265696  -MSE  7.00309744071735  -Training accuracy : 0.5589623\n",
      "epoch:  37  -  cost : 0.7316573  -MSE  7.019310037339582  -Training accuracy : 0.5613208\n",
      "epoch:  38  -  cost : 0.7306922  -MSE  7.034504585255237  -Training accuracy : 0.5625\n",
      "epoch:  39  -  cost : 0.7297561  -MSE  7.048825259464241  -Training accuracy : 0.5636792\n",
      "epoch:  40  -  cost : 0.7288506  -MSE  7.062256158734849  -Training accuracy : 0.5648585\n",
      "epoch:  41  -  cost : 0.7279797  -MSE  7.074924151345231  -Training accuracy : 0.5648585\n",
      "epoch:  42  -  cost : 0.7271354  -MSE  7.086805349606985  -Training accuracy : 0.5660377\n",
      "epoch:  43  -  cost : 0.7263086  -MSE  7.097963129509146  -Training accuracy : 0.5683962\n",
      "epoch:  44  -  cost : 0.7255041  -MSE  7.108522958907994  -Training accuracy : 0.571934\n",
      "epoch:  45  -  cost : 0.72471696  -MSE  7.118540551618821  -Training accuracy : 0.5731132\n",
      "epoch:  46  -  cost : 0.7239578  -MSE  7.127929521261824  -Training accuracy : 0.5766509\n",
      "epoch:  47  -  cost : 0.7232243  -MSE  7.136708410317286  -Training accuracy : 0.5778302\n",
      "epoch:  48  -  cost : 0.7225055  -MSE  7.144978605680286  -Training accuracy : 0.5801887\n",
      "epoch:  49  -  cost : 0.72180545  -MSE  7.152720531568479  -Training accuracy : 0.5801887\n",
      "epoch:  50  -  cost : 0.72112733  -MSE  7.159959000715239  -Training accuracy : 0.5801887\n",
      "epoch:  51  -  cost : 0.72046244  -MSE  7.166780379821123  -Training accuracy : 0.5813679\n",
      "epoch:  52  -  cost : 0.7198138  -MSE  7.173202265523652  -Training accuracy : 0.5813679\n",
      "epoch:  53  -  cost : 0.719182  -MSE  7.179227482131948  -Training accuracy : 0.5813679\n",
      "epoch:  54  -  cost : 0.71856487  -MSE  7.184892806648403  -Training accuracy : 0.5813679\n",
      "epoch:  55  -  cost : 0.7179603  -MSE  7.190255025033862  -Training accuracy : 0.5813679\n",
      "epoch:  56  -  cost : 0.7173623  -MSE  7.195325305212649  -Training accuracy : 0.5813679\n",
      "epoch:  57  -  cost : 0.7167737  -MSE  7.200125421535528  -Training accuracy : 0.5813679\n",
      "epoch:  58  -  cost : 0.71619374  -MSE  7.2046672702166035  -Training accuracy : 0.5813679\n",
      "epoch:  59  -  cost : 0.7156247  -MSE  7.208962891651002  -Training accuracy : 0.5813679\n",
      "epoch:  60  -  cost : 0.71506375  -MSE  7.2130626823450115  -Training accuracy : 0.5813679\n",
      "epoch:  61  -  cost : 0.7145104  -MSE  7.2169783630921005  -Training accuracy : 0.5813679\n",
      "epoch:  62  -  cost : 0.71396077  -MSE  7.220712186216392  -Training accuracy : 0.5813679\n",
      "epoch:  63  -  cost : 0.7134163  -MSE  7.2242669359866465  -Training accuracy : 0.5813679\n",
      "epoch:  64  -  cost : 0.71287537  -MSE  7.22766173107269  -Training accuracy : 0.5813679\n",
      "epoch:  65  -  cost : 0.71233934  -MSE  7.230908338094483  -Training accuracy : 0.5813679\n",
      "epoch:  66  -  cost : 0.7118069  -MSE  7.234048268579822  -Training accuracy : 0.5813679\n",
      "epoch:  67  -  cost : 0.7112805  -MSE  7.237051918204459  -Training accuracy : 0.5813679\n",
      "epoch:  68  -  cost : 0.71076006  -MSE  7.239934803968105  -Training accuracy : 0.5813679\n",
      "epoch:  69  -  cost : 0.71024334  -MSE  7.242733716339794  -Training accuracy : 0.5813679\n",
      "epoch:  70  -  cost : 0.7097296  -MSE  7.2454330261861  -Training accuracy : 0.5813679\n",
      "epoch:  71  -  cost : 0.70921975  -MSE  7.2480644616878225  -Training accuracy : 0.5813679\n",
      "epoch:  72  -  cost : 0.7087136  -MSE  7.250639197104331  -Training accuracy : 0.5813679\n",
      "epoch:  73  -  cost : 0.7082122  -MSE  7.253116799960229  -Training accuracy : 0.5813679\n",
      "epoch:  74  -  cost : 0.70771664  -MSE  7.255540223546359  -Training accuracy : 0.5813679\n",
      "epoch:  75  -  cost : 0.70722574  -MSE  7.257892148655158  -Training accuracy : 0.5813679\n",
      "epoch:  76  -  cost : 0.7067399  -MSE  7.260171332328469  -Training accuracy : 0.5813679\n",
      "epoch:  77  -  cost : 0.70625716  -MSE  7.262394166059439  -Training accuracy : 0.5813679\n",
      "epoch:  78  -  cost : 0.70577866  -MSE  7.264557201779397  -Training accuracy : 0.5813679\n",
      "epoch:  79  -  cost : 0.7053038  -MSE  7.2666716371315365  -Training accuracy : 0.5813679\n",
      "epoch:  80  -  cost : 0.70483255  -MSE  7.268741692128563  -Training accuracy : 0.5813679\n",
      "epoch:  81  -  cost : 0.7043645  -MSE  7.270769912863108  -Training accuracy : 0.5813679\n",
      "epoch:  82  -  cost : 0.7038994  -MSE  7.272759045373803  -Training accuracy : 0.5813679\n",
      "epoch:  83  -  cost : 0.703437  -MSE  7.274720537193809  -Training accuracy : 0.5813679\n",
      "epoch:  84  -  cost : 0.7029775  -MSE  7.276640725059595  -Training accuracy : 0.5813679\n",
      "epoch:  85  -  cost : 0.70252055  -MSE  7.27853677266947  -Training accuracy : 0.5813679\n",
      "epoch:  86  -  cost : 0.7020661  -MSE  7.280409051255246  -Training accuracy : 0.5813679\n",
      "epoch:  87  -  cost : 0.70161396  -MSE  7.282262150483351  -Training accuracy : 0.5813679\n",
      "epoch:  88  -  cost : 0.7011642  -MSE  7.284098730917225  -Training accuracy : 0.5813679\n",
      "epoch:  89  -  cost : 0.7007166  -MSE  7.285920073571445  -Training accuracy : 0.5813679\n",
      "epoch:  90  -  cost : 0.7002715  -MSE  7.287723444843499  -Training accuracy : 0.5813679\n",
      "epoch:  91  -  cost : 0.6998281  -MSE  7.289539717564332  -Training accuracy : 0.5813679\n",
      "epoch:  92  -  cost : 0.699387  -MSE  7.291342532316884  -Training accuracy : 0.5813679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  93  -  cost : 0.6989478  -MSE  7.293158717560135  -Training accuracy : 0.5813679\n",
      "epoch:  94  -  cost : 0.6985108  -MSE  7.294962774266143  -Training accuracy : 0.5813679\n",
      "epoch:  95  -  cost : 0.6980762  -MSE  7.296754620115797  -Training accuracy : 0.5813679\n",
      "epoch:  96  -  cost : 0.69764364  -MSE  7.298539672568783  -Training accuracy : 0.5813679\n",
      "epoch:  97  -  cost : 0.69721293  -MSE  7.30031780824596  -Training accuracy : 0.5813679\n",
      "epoch:  98  -  cost : 0.6967839  -MSE  7.30211577234286  -Training accuracy : 0.5813679\n",
      "epoch:  99  -  cost : 0.696357  -MSE  7.303902070321639  -Training accuracy : 0.5813679\n",
      "epoch:  100  -  cost : 0.6959322  -MSE  7.305683716724196  -Training accuracy : 0.5813679\n",
      "epoch:  101  -  cost : 0.69550943  -MSE  7.307461406673834  -Training accuracy : 0.5813679\n",
      "epoch:  102  -  cost : 0.6950882  -MSE  7.309244335618056  -Training accuracy : 0.5813679\n",
      "epoch:  103  -  cost : 0.6946679  -MSE  7.311073352471014  -Training accuracy : 0.5813679\n",
      "epoch:  104  -  cost : 0.69424933  -MSE  7.3129000490538525  -Training accuracy : 0.5825472\n",
      "epoch:  105  -  cost : 0.6938317  -MSE  7.314767719192641  -Training accuracy : 0.5825472\n",
      "epoch:  106  -  cost : 0.6934155  -MSE  7.3166495305443595  -Training accuracy : 0.5825472\n",
      "epoch:  107  -  cost : 0.6930011  -MSE  7.318526724735937  -Training accuracy : 0.5849057\n",
      "epoch:  108  -  cost : 0.6925889  -MSE  7.3203972517897995  -Training accuracy : 0.5860849\n",
      "epoch:  109  -  cost : 0.69217813  -MSE  7.322291415810041  -Training accuracy : 0.5860849\n",
      "epoch:  110  -  cost : 0.6917697  -MSE  7.324177604208068  -Training accuracy : 0.5872642\n",
      "epoch:  111  -  cost : 0.69136333  -MSE  7.326044726473209  -Training accuracy : 0.5872642\n",
      "epoch:  112  -  cost : 0.69095916  -MSE  7.327906222574044  -Training accuracy : 0.5896226\n",
      "epoch:  113  -  cost : 0.69055694  -MSE  7.329749812491829  -Training accuracy : 0.5896226\n",
      "epoch:  114  -  cost : 0.69015676  -MSE  7.331597953444912  -Training accuracy : 0.5896226\n",
      "epoch:  115  -  cost : 0.68975794  -MSE  7.333444392783115  -Training accuracy : 0.5908019\n",
      "epoch:  116  -  cost : 0.6893601  -MSE  7.33532923321364  -Training accuracy : 0.5908019\n",
      "epoch:  117  -  cost : 0.6889639  -MSE  7.337220013309531  -Training accuracy : 0.5908019\n",
      "epoch:  118  -  cost : 0.6885694  -MSE  7.339103446762042  -Training accuracy : 0.5908019\n",
      "epoch:  119  -  cost : 0.688176  -MSE  7.3410027164555  -Training accuracy : 0.5919811\n",
      "epoch:  120  -  cost : 0.68778414  -MSE  7.342912343948219  -Training accuracy : 0.5919811\n",
      "epoch:  121  -  cost : 0.68739396  -MSE  7.344823967848721  -Training accuracy : 0.5943396\n",
      "epoch:  122  -  cost : 0.6870055  -MSE  7.346737652680546  -Training accuracy : 0.6002358\n",
      "epoch:  123  -  cost : 0.68661886  -MSE  7.348641662372654  -Training accuracy : 0.6014151\n",
      "epoch:  124  -  cost : 0.68623364  -MSE  7.350545247547178  -Training accuracy : 0.6037736\n",
      "epoch:  125  -  cost : 0.68584985  -MSE  7.352475404235887  -Training accuracy : 0.6049528\n",
      "epoch:  126  -  cost : 0.68546754  -MSE  7.354416696891802  -Training accuracy : 0.6061321\n",
      "epoch:  127  -  cost : 0.6850869  -MSE  7.356359922029433  -Training accuracy : 0.6061321\n",
      "epoch:  128  -  cost : 0.68470734  -MSE  7.358330291134235  -Training accuracy : 0.6073113\n",
      "epoch:  129  -  cost : 0.68432957  -MSE  7.3603035359567315  -Training accuracy : 0.6084906\n",
      "epoch:  130  -  cost : 0.68395317  -MSE  7.3622791150984614  -Training accuracy : 0.6096698\n",
      "epoch:  131  -  cost : 0.6835781  -MSE  7.364279201244854  -Training accuracy : 0.6120283\n",
      "epoch:  132  -  cost : 0.6832043  -MSE  7.36628277450846  -Training accuracy : 0.615566\n",
      "epoch:  133  -  cost : 0.6828312  -MSE  7.368333746906489  -Training accuracy : 0.6167453\n",
      "epoch:  134  -  cost : 0.68246  -MSE  7.370385504098458  -Training accuracy : 0.6191038\n",
      "epoch:  135  -  cost : 0.6820916  -MSE  7.372420404905127  -Training accuracy : 0.620283\n",
      "epoch:  136  -  cost : 0.6817257  -MSE  7.374392079998085  -Training accuracy : 0.6214623\n",
      "epoch:  137  -  cost : 0.6813608  -MSE  7.376389709634988  -Training accuracy : 0.6226415\n",
      "epoch:  138  -  cost : 0.6809976  -MSE  7.378390990630105  -Training accuracy : 0.6226415\n",
      "epoch:  139  -  cost : 0.68063587  -MSE  7.380397166201206  -Training accuracy : 0.6226415\n",
      "epoch:  140  -  cost : 0.6802758  -MSE  7.382402655377858  -Training accuracy : 0.6238208\n",
      "epoch:  141  -  cost : 0.6799171  -MSE  7.384411742923051  -Training accuracy : 0.6273585\n",
      "epoch:  142  -  cost : 0.67955995  -MSE  7.386424724647773  -Training accuracy : 0.6285377\n",
      "epoch:  143  -  cost : 0.6792044  -MSE  7.388435877638119  -Training accuracy : 0.629717\n",
      "epoch:  144  -  cost : 0.6788503  -MSE  7.390433757557569  -Training accuracy : 0.6308962\n",
      "epoch:  145  -  cost : 0.67849684  -MSE  7.392467364974312  -Training accuracy : 0.6308962\n",
      "epoch:  146  -  cost : 0.67814493  -MSE  7.3945038289968155  -Training accuracy : 0.6308962\n",
      "epoch:  147  -  cost : 0.67779446  -MSE  7.39654425360894  -Training accuracy : 0.6320755\n",
      "epoch:  148  -  cost : 0.67744553  -MSE  7.3985885013554356  -Training accuracy : 0.6332547\n",
      "epoch:  149  -  cost : 0.67709833  -MSE  7.400623017000451  -Training accuracy : 0.6332547\n",
      "epoch:  150  -  cost : 0.6767525  -MSE  7.402652132144191  -Training accuracy : 0.6356132\n",
      "epoch:  151  -  cost : 0.6764083  -MSE  7.40467229788633  -Training accuracy : 0.6367925\n",
      "epoch:  152  -  cost : 0.67606527  -MSE  7.406697683802465  -Training accuracy : 0.6367925\n",
      "epoch:  153  -  cost : 0.6757238  -MSE  7.408726078001688  -Training accuracy : 0.6367925\n",
      "epoch:  154  -  cost : 0.67538303  -MSE  7.410779576012318  -Training accuracy : 0.6391509\n",
      "epoch:  155  -  cost : 0.6750436  -MSE  7.412837069837166  -Training accuracy : 0.6391509\n",
      "epoch:  156  -  cost : 0.6747056  -MSE  7.414888923398083  -Training accuracy : 0.6403302\n",
      "epoch:  157  -  cost : 0.6743692  -MSE  7.416931069555626  -Training accuracy : 0.6438679\n",
      "epoch:  158  -  cost : 0.67403424  -MSE  7.418977998267432  -Training accuracy : 0.6450472\n",
      "epoch:  159  -  cost : 0.6737008  -MSE  7.421028083624865  -Training accuracy : 0.6485849\n",
      "epoch:  160  -  cost : 0.67336875  -MSE  7.423063928730849  -Training accuracy : 0.6485849\n",
      "epoch:  161  -  cost : 0.6730378  -MSE  7.425101807137225  -Training accuracy : 0.6497642\n",
      "epoch:  162  -  cost : 0.6727077  -MSE  7.427163086721866  -Training accuracy : 0.6497642\n",
      "epoch:  163  -  cost : 0.6723782  -MSE  7.429250488545275  -Training accuracy : 0.6497642\n",
      "epoch:  164  -  cost : 0.67204976  -MSE  7.431361499239177  -Training accuracy : 0.6497642\n",
      "epoch:  165  -  cost : 0.67172205  -MSE  7.433475589390329  -Training accuracy : 0.6509434\n",
      "epoch:  166  -  cost : 0.67139566  -MSE  7.435612915390277  -Training accuracy : 0.6509434\n",
      "epoch:  167  -  cost : 0.6710708  -MSE  7.437735158736308  -Training accuracy : 0.6509434\n",
      "epoch:  168  -  cost : 0.6707473  -MSE  7.439860646379718  -Training accuracy : 0.6509434\n",
      "epoch:  169  -  cost : 0.6704251  -MSE  7.441968508632116  -Training accuracy : 0.6544811\n",
      "epoch:  170  -  cost : 0.67010427  -MSE  7.4440805309956755  -Training accuracy : 0.6556604\n",
      "epoch:  171  -  cost : 0.66978455  -MSE  7.446195061736545  -Training accuracy : 0.6568396\n",
      "epoch:  172  -  cost : 0.66946596  -MSE  7.448313040540602  -Training accuracy : 0.6568396\n",
      "epoch:  173  -  cost : 0.669149  -MSE  7.450421899164768  -Training accuracy : 0.6580189\n",
      "epoch:  174  -  cost : 0.668833  -MSE  7.452534662982115  -Training accuracy : 0.6591981\n",
      "epoch:  175  -  cost : 0.6685182  -MSE  7.454651658669356  -Training accuracy : 0.6603774\n",
      "epoch:  176  -  cost : 0.66820383  -MSE  7.456788726566165  -Training accuracy : 0.6615566\n",
      "epoch:  177  -  cost : 0.66789037  -MSE  7.458947417706487  -Training accuracy : 0.6615566\n",
      "epoch:  178  -  cost : 0.6675778  -MSE  7.461108494214251  -Training accuracy : 0.6615566\n",
      "epoch:  179  -  cost : 0.66726625  -MSE  7.463277731941002  -Training accuracy : 0.6615566\n",
      "epoch:  180  -  cost : 0.66695607  -MSE  7.465453628505238  -Training accuracy : 0.6627358\n",
      "epoch:  181  -  cost : 0.6666467  -MSE  7.467631637354286  -Training accuracy : 0.6639151\n",
      "epoch:  182  -  cost : 0.6663385  -MSE  7.469811799504928  -Training accuracy : 0.6650943\n",
      "epoch:  183  -  cost : 0.6660311  -MSE  7.4719947161858  -Training accuracy : 0.6650943\n",
      "epoch:  184  -  cost : 0.6657241  -MSE  7.474219272251002  -Training accuracy : 0.6650943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  185  -  cost : 0.6654182  -MSE  7.4764419126262185  -Training accuracy : 0.6650943\n",
      "epoch:  186  -  cost : 0.6651132  -MSE  7.478665026637657  -Training accuracy : 0.6662736\n",
      "epoch:  187  -  cost : 0.6648092  -MSE  7.480889155630217  -Training accuracy : 0.6662736\n",
      "epoch:  188  -  cost : 0.664506  -MSE  7.483130336470079  -Training accuracy : 0.6674528\n",
      "epoch:  189  -  cost : 0.6642039  -MSE  7.485373667580777  -Training accuracy : 0.6674528\n",
      "epoch:  190  -  cost : 0.66390294  -MSE  7.487617641980379  -Training accuracy : 0.6698113\n",
      "epoch:  191  -  cost : 0.6636033  -MSE  7.489838237797647  -Training accuracy : 0.6698113\n",
      "epoch:  192  -  cost : 0.6633047  -MSE  7.492068229513617  -Training accuracy : 0.6698113\n",
      "epoch:  193  -  cost : 0.6630072  -MSE  7.494287337141536  -Training accuracy : 0.6698113\n",
      "epoch:  194  -  cost : 0.6627108  -MSE  7.4965091391808905  -Training accuracy : 0.6698113\n",
      "epoch:  195  -  cost : 0.66241527  -MSE  7.498731534570126  -Training accuracy : 0.6698113\n",
      "epoch:  196  -  cost : 0.66212016  -MSE  7.5009771382896195  -Training accuracy : 0.6698113\n",
      "epoch:  197  -  cost : 0.6618258  -MSE  7.503242530612297  -Training accuracy : 0.6709906\n",
      "epoch:  198  -  cost : 0.66153276  -MSE  7.505512706269921  -Training accuracy : 0.6709906\n",
      "epoch:  199  -  cost : 0.66124076  -MSE  7.507748151688444  -Training accuracy : 0.6709906\n",
      "epoch:  200  -  cost : 0.66095  -MSE  7.509997317112594  -Training accuracy : 0.6709906\n",
      "epoch:  201  -  cost : 0.6606602  -MSE  7.512236788352352  -Training accuracy : 0.6709906\n",
      "epoch:  202  -  cost : 0.66037136  -MSE  7.514476930007421  -Training accuracy : 0.6709906\n",
      "epoch:  203  -  cost : 0.66008353  -MSE  7.516725653803957  -Training accuracy : 0.6721698\n",
      "epoch:  204  -  cost : 0.65979654  -MSE  7.518954839467524  -Training accuracy : 0.6721698\n",
      "epoch:  205  -  cost : 0.6595103  -MSE  7.521207284152394  -Training accuracy : 0.6733491\n",
      "epoch:  206  -  cost : 0.65922475  -MSE  7.523460517779887  -Training accuracy : 0.6745283\n",
      "epoch:  207  -  cost : 0.65893996  -MSE  7.525733548505591  -Training accuracy : 0.6745283\n",
      "epoch:  208  -  cost : 0.6586558  -MSE  7.52800350710615  -Training accuracy : 0.6745283\n",
      "epoch:  209  -  cost : 0.658373  -MSE  7.530264979405083  -Training accuracy : 0.6757075\n",
      "epoch:  210  -  cost : 0.65809083  -MSE  7.532528415318272  -Training accuracy : 0.6757075\n",
      "epoch:  211  -  cost : 0.65780914  -MSE  7.534811762891326  -Training accuracy : 0.6768868\n",
      "epoch:  212  -  cost : 0.6575283  -MSE  7.537096163137476  -Training accuracy : 0.6792453\n",
      "epoch:  213  -  cost : 0.6572486  -MSE  7.539377397741616  -Training accuracy : 0.6792453\n",
      "epoch:  214  -  cost : 0.6569705  -MSE  7.541635227015743  -Training accuracy : 0.6792453\n",
      "epoch:  215  -  cost : 0.6566932  -MSE  7.5438675558895225  -Training accuracy : 0.6804245\n",
      "epoch:  216  -  cost : 0.65641683  -MSE  7.546120250894482  -Training accuracy : 0.682783\n",
      "epoch:  217  -  cost : 0.6561407  -MSE  7.548394860418386  -Training accuracy : 0.682783\n",
      "epoch:  218  -  cost : 0.6558652  -MSE  7.550669467849869  -Training accuracy : 0.682783\n",
      "epoch:  219  -  cost : 0.65559053  -MSE  7.552942077552977  -Training accuracy : 0.6839623\n",
      "epoch:  220  -  cost : 0.65531677  -MSE  7.555212946164898  -Training accuracy : 0.6839623\n",
      "epoch:  221  -  cost : 0.6550437  -MSE  7.557484352453125  -Training accuracy : 0.6839623\n",
      "epoch:  222  -  cost : 0.6547713  -MSE  7.559751021096257  -Training accuracy : 0.6839623\n",
      "epoch:  223  -  cost : 0.65449977  -MSE  7.562016307105414  -Training accuracy : 0.6851415\n",
      "epoch:  224  -  cost : 0.6542289  -MSE  7.564282431441746  -Training accuracy : 0.6863208\n",
      "epoch:  225  -  cost : 0.65395916  -MSE  7.566544658695099  -Training accuracy : 0.6863208\n",
      "epoch:  226  -  cost : 0.65369046  -MSE  7.5687671990154515  -Training accuracy : 0.6875\n",
      "epoch:  227  -  cost : 0.6534223  -MSE  7.571008308403073  -Training accuracy : 0.6875\n",
      "epoch:  228  -  cost : 0.65315497  -MSE  7.573245802699579  -Training accuracy : 0.6875\n",
      "epoch:  229  -  cost : 0.65288794  -MSE  7.57550198222263  -Training accuracy : 0.6875\n",
      "epoch:  230  -  cost : 0.6526216  -MSE  7.5777645290418025  -Training accuracy : 0.6875\n",
      "epoch:  231  -  cost : 0.65235597  -MSE  7.580013697065194  -Training accuracy : 0.6875\n",
      "epoch:  232  -  cost : 0.652091  -MSE  7.582260389056037  -Training accuracy : 0.6886792\n",
      "epoch:  233  -  cost : 0.6518262  -MSE  7.584535605058545  -Training accuracy : 0.6886792\n",
      "epoch:  234  -  cost : 0.6515621  -MSE  7.586810742760052  -Training accuracy : 0.6898585\n",
      "epoch:  235  -  cost : 0.65129876  -MSE  7.589085407696566  -Training accuracy : 0.6898585\n",
      "epoch:  236  -  cost : 0.6510361  -MSE  7.59136026711962  -Training accuracy : 0.6898585\n",
      "epoch:  237  -  cost : 0.6507741  -MSE  7.593623247506924  -Training accuracy : 0.6898585\n",
      "epoch:  238  -  cost : 0.6505126  -MSE  7.595903795955466  -Training accuracy : 0.6898585\n",
      "epoch:  239  -  cost : 0.6502518  -MSE  7.598184532874354  -Training accuracy : 0.6910377\n",
      "epoch:  240  -  cost : 0.6499916  -MSE  7.600464535073594  -Training accuracy : 0.692217\n",
      "epoch:  241  -  cost : 0.64973205  -MSE  7.60273300919201  -Training accuracy : 0.692217\n",
      "epoch:  242  -  cost : 0.6494738  -MSE  7.604980477278868  -Training accuracy : 0.692217\n",
      "epoch:  243  -  cost : 0.64921683  -MSE  7.607206124154652  -Training accuracy : 0.692217\n",
      "epoch:  244  -  cost : 0.64896077  -MSE  7.609457163109293  -Training accuracy : 0.692217\n",
      "epoch:  245  -  cost : 0.6487048  -MSE  7.611734641738152  -Training accuracy : 0.692217\n",
      "epoch:  246  -  cost : 0.6484492  -MSE  7.614029107512703  -Training accuracy : 0.692217\n",
      "epoch:  247  -  cost : 0.6481941  -MSE  7.616325871559802  -Training accuracy : 0.6933962\n",
      "epoch:  248  -  cost : 0.64793915  -MSE  7.618631540706918  -Training accuracy : 0.6933962\n",
      "epoch:  249  -  cost : 0.64768475  -MSE  7.620926777509668  -Training accuracy : 0.6933962\n",
      "epoch:  250  -  cost : 0.6474307  -MSE  7.623231231715033  -Training accuracy : 0.6933962\n",
      "epoch:  251  -  cost : 0.6471769  -MSE  7.625543902851101  -Training accuracy : 0.6945755\n",
      "epoch:  252  -  cost : 0.6469235  -MSE  7.6278662523059  -Training accuracy : 0.6945755\n",
      "epoch:  253  -  cost : 0.64667064  -MSE  7.630187947296984  -Training accuracy : 0.6945755\n",
      "epoch:  254  -  cost : 0.6464184  -MSE  7.632507315234478  -Training accuracy : 0.6945755\n",
      "epoch:  255  -  cost : 0.6461664  -MSE  7.634826237830588  -Training accuracy : 0.6945755\n",
      "epoch:  256  -  cost : 0.6459143  -MSE  7.637174524746467  -Training accuracy : 0.6945755\n",
      "epoch:  257  -  cost : 0.6456629  -MSE  7.6395315595587725  -Training accuracy : 0.6945755\n",
      "epoch:  258  -  cost : 0.64541215  -MSE  7.641928867678536  -Training accuracy : 0.6945755\n",
      "epoch:  259  -  cost : 0.6451617  -MSE  7.6443369312436715  -Training accuracy : 0.6945755\n",
      "epoch:  260  -  cost : 0.64491165  -MSE  7.646754793789275  -Training accuracy : 0.6945755\n",
      "epoch:  261  -  cost : 0.64466226  -MSE  7.649159758068723  -Training accuracy : 0.6957547\n",
      "epoch:  262  -  cost : 0.6444135  -MSE  7.651559281214229  -Training accuracy : 0.6957547\n",
      "epoch:  263  -  cost : 0.6441649  -MSE  7.6539714617050825  -Training accuracy : 0.6957547\n",
      "epoch:  264  -  cost : 0.6439164  -MSE  7.6563926104236755  -Training accuracy : 0.6957547\n",
      "epoch:  265  -  cost : 0.6436684  -MSE  7.6588252078045205  -Training accuracy : 0.6957547\n",
      "epoch:  266  -  cost : 0.6434208  -MSE  7.661254783007091  -Training accuracy : 0.6957547\n",
      "epoch:  267  -  cost : 0.6431729  -MSE  7.663714434932252  -Training accuracy : 0.6957547\n",
      "epoch:  268  -  cost : 0.6429253  -MSE  7.666195598594806  -Training accuracy : 0.6957547\n",
      "epoch:  269  -  cost : 0.64267814  -MSE  7.668674095250332  -Training accuracy : 0.696934\n",
      "epoch:  270  -  cost : 0.642431  -MSE  7.671154324524135  -Training accuracy : 0.696934\n",
      "epoch:  271  -  cost : 0.64218396  -MSE  7.673659254481393  -Training accuracy : 0.6981132\n",
      "epoch:  272  -  cost : 0.6419369  -MSE  7.676174748274745  -Training accuracy : 0.6981132\n",
      "epoch:  273  -  cost : 0.64168966  -MSE  7.678725236135845  -Training accuracy : 0.6992925\n",
      "epoch:  274  -  cost : 0.64144284  -MSE  7.681276416838543  -Training accuracy : 0.6992925\n",
      "epoch:  275  -  cost : 0.64119637  -MSE  7.683829818236599  -Training accuracy : 0.6992925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  276  -  cost : 0.6409501  -MSE  7.68639348259882  -Training accuracy : 0.6992925\n",
      "epoch:  277  -  cost : 0.6407045  -MSE  7.688945742318658  -Training accuracy : 0.6992925\n",
      "epoch:  278  -  cost : 0.6404596  -MSE  7.691486143258786  -Training accuracy : 0.6992925\n",
      "epoch:  279  -  cost : 0.64021516  -MSE  7.694037193352003  -Training accuracy : 0.6992925\n",
      "epoch:  280  -  cost : 0.6399712  -MSE  7.696574867585189  -Training accuracy : 0.6992925\n",
      "epoch:  281  -  cost : 0.63972795  -MSE  7.699096935282812  -Training accuracy : 0.6992925\n",
      "epoch:  282  -  cost : 0.6394851  -MSE  7.701607900025246  -Training accuracy : 0.6992925\n",
      "epoch:  283  -  cost : 0.6392425  -MSE  7.704125114718174  -Training accuracy : 0.7004717\n",
      "epoch:  284  -  cost : 0.63900054  -MSE  7.706692906682653  -Training accuracy : 0.7004717\n",
      "epoch:  285  -  cost : 0.63875896  -MSE  7.70924921897962  -Training accuracy : 0.7004717\n",
      "epoch:  286  -  cost : 0.63851786  -MSE  7.711813233962563  -Training accuracy : 0.7004717\n",
      "epoch:  287  -  cost : 0.6382775  -MSE  7.714364916634391  -Training accuracy : 0.7004717\n",
      "epoch:  288  -  cost : 0.6380377  -MSE  7.716908480010136  -Training accuracy : 0.7004717\n",
      "epoch:  289  -  cost : 0.6377979  -MSE  7.719472807010733  -Training accuracy : 0.7004717\n",
      "epoch:  290  -  cost : 0.6375588  -MSE  7.72201693287323  -Training accuracy : 0.7004717\n",
      "epoch:  291  -  cost : 0.63732046  -MSE  7.724555465580729  -Training accuracy : 0.7004717\n",
      "epoch:  292  -  cost : 0.6370823  -MSE  7.727085573295053  -Training accuracy : 0.7004717\n",
      "epoch:  293  -  cost : 0.63684434  -MSE  7.729625335611284  -Training accuracy : 0.7016509\n",
      "epoch:  294  -  cost : 0.6366067  -MSE  7.732172450241572  -Training accuracy : 0.7016509\n",
      "epoch:  295  -  cost : 0.6363695  -MSE  7.734717007202572  -Training accuracy : 0.7016509\n",
      "epoch:  296  -  cost : 0.6361329  -MSE  7.737244177135578  -Training accuracy : 0.7016509\n",
      "epoch:  297  -  cost : 0.6358966  -MSE  7.739752903784238  -Training accuracy : 0.7016509\n",
      "epoch:  298  -  cost : 0.6356606  -MSE  7.742281099187665  -Training accuracy : 0.7028302\n",
      "epoch:  299  -  cost : 0.635425  -MSE  7.744799105204377  -Training accuracy : 0.7028302\n",
      "epoch:  300  -  cost : 0.6351897  -MSE  7.747330653901011  -Training accuracy : 0.7028302\n",
      "epoch:  301  -  cost : 0.6349546  -MSE  7.749853323563167  -Training accuracy : 0.7028302\n",
      "epoch:  302  -  cost : 0.63471913  -MSE  7.752406010053898  -Training accuracy : 0.7028302\n",
      "epoch:  303  -  cost : 0.6344841  -MSE  7.754959161131586  -Training accuracy : 0.7040094\n",
      "epoch:  304  -  cost : 0.63424957  -MSE  7.757508963867141  -Training accuracy : 0.7040094\n",
      "epoch:  305  -  cost : 0.6340154  -MSE  7.76006004314115  -Training accuracy : 0.7040094\n",
      "epoch:  306  -  cost : 0.63378155  -MSE  7.762608481639031  -Training accuracy : 0.7051887\n",
      "epoch:  307  -  cost : 0.63354796  -MSE  7.765167403705545  -Training accuracy : 0.7051887\n",
      "epoch:  308  -  cost : 0.63331455  -MSE  7.767732776128421  -Training accuracy : 0.7051887\n",
      "epoch:  309  -  cost : 0.63308156  -MSE  7.770290519651439  -Training accuracy : 0.7051887\n",
      "epoch:  310  -  cost : 0.6328486  -MSE  7.772852299569003  -Training accuracy : 0.7063679\n",
      "epoch:  311  -  cost : 0.63261586  -MSE  7.775422317052771  -Training accuracy : 0.7063679\n",
      "epoch:  312  -  cost : 0.6323835  -MSE  7.777989540260857  -Training accuracy : 0.7063679\n",
      "epoch:  313  -  cost : 0.6321517  -MSE  7.780557129685949  -Training accuracy : 0.7063679\n",
      "epoch:  314  -  cost : 0.63191974  -MSE  7.783125340874212  -Training accuracy : 0.7063679\n",
      "epoch:  315  -  cost : 0.631688  -MSE  7.785710543234924  -Training accuracy : 0.7075472\n",
      "epoch:  316  -  cost : 0.63145703  -MSE  7.78827245279363  -Training accuracy : 0.7075472\n",
      "epoch:  317  -  cost : 0.6312264  -MSE  7.790846137340085  -Training accuracy : 0.7075472\n",
      "epoch:  318  -  cost : 0.6309963  -MSE  7.793406983031111  -Training accuracy : 0.7087264\n",
      "epoch:  319  -  cost : 0.63076645  -MSE  7.795965173332554  -Training accuracy : 0.7087264\n",
      "epoch:  320  -  cost : 0.6305368  -MSE  7.79852423294183  -Training accuracy : 0.7087264\n",
      "epoch:  321  -  cost : 0.6303075  -MSE  7.801091932301977  -Training accuracy : 0.7087264\n",
      "epoch:  322  -  cost : 0.63007843  -MSE  7.80365734689978  -Training accuracy : 0.7087264\n",
      "epoch:  323  -  cost : 0.62984973  -MSE  7.806220303807057  -Training accuracy : 0.7087264\n",
      "epoch:  324  -  cost : 0.62962145  -MSE  7.808773360660235  -Training accuracy : 0.7087264\n",
      "epoch:  325  -  cost : 0.62939346  -MSE  7.811326049287871  -Training accuracy : 0.7087264\n",
      "epoch:  326  -  cost : 0.6291657  -MSE  7.813877221445832  -Training accuracy : 0.7099057\n",
      "epoch:  327  -  cost : 0.6289384  -MSE  7.816426273221265  -Training accuracy : 0.7110849\n",
      "epoch:  328  -  cost : 0.6287112  -MSE  7.8189670539652365  -Training accuracy : 0.7110849\n",
      "epoch:  329  -  cost : 0.62848383  -MSE  7.821540816896643  -Training accuracy : 0.7110849\n",
      "epoch:  330  -  cost : 0.6282561  -MSE  7.824133455513364  -Training accuracy : 0.7122642\n",
      "epoch:  331  -  cost : 0.6280288  -MSE  7.826723917570657  -Training accuracy : 0.7134434\n",
      "epoch:  332  -  cost : 0.6278016  -MSE  7.829314136288954  -Training accuracy : 0.7146226\n",
      "epoch:  333  -  cost : 0.62757486  -MSE  7.831915792612073  -Training accuracy : 0.7146226\n",
      "epoch:  334  -  cost : 0.6273481  -MSE  7.834515097568131  -Training accuracy : 0.7146226\n",
      "epoch:  335  -  cost : 0.62712175  -MSE  7.837111952470709  -Training accuracy : 0.7146226\n",
      "epoch:  336  -  cost : 0.62689555  -MSE  7.839707472507439  -Training accuracy : 0.7146226\n",
      "epoch:  337  -  cost : 0.62666935  -MSE  7.842309184829508  -Training accuracy : 0.7146226\n",
      "epoch:  338  -  cost : 0.62644374  -MSE  7.844909728939827  -Training accuracy : 0.7146226\n",
      "epoch:  339  -  cost : 0.6262184  -MSE  7.847507525484464  -Training accuracy : 0.7146226\n",
      "epoch:  340  -  cost : 0.62599325  -MSE  7.850102770891206  -Training accuracy : 0.7146226\n",
      "epoch:  341  -  cost : 0.62576824  -MSE  7.852696965793636  -Training accuracy : 0.7146226\n",
      "epoch:  342  -  cost : 0.62554365  -MSE  7.855288955386298  -Training accuracy : 0.7146226\n",
      "epoch:  343  -  cost : 0.62531936  -MSE  7.857892893498126  -Training accuracy : 0.7146226\n",
      "epoch:  344  -  cost : 0.62509537  -MSE  7.860496875614419  -Training accuracy : 0.7158019\n",
      "epoch:  345  -  cost : 0.6248716  -MSE  7.8630975137313115  -Training accuracy : 0.7158019\n",
      "epoch:  346  -  cost : 0.624648  -MSE  7.865697819067106  -Training accuracy : 0.7099057\n",
      "epoch:  347  -  cost : 0.62442464  -MSE  7.8682881503953706  -Training accuracy : 0.7099057\n",
      "epoch:  348  -  cost : 0.6242015  -MSE  7.870875183583476  -Training accuracy : 0.7099057\n",
      "epoch:  349  -  cost : 0.6239784  -MSE  7.873469035467858  -Training accuracy : 0.7099057\n",
      "epoch:  350  -  cost : 0.6237558  -MSE  7.876046757057388  -Training accuracy : 0.7099057\n",
      "epoch:  351  -  cost : 0.6235334  -MSE  7.878618266077827  -Training accuracy : 0.7099057\n",
      "epoch:  352  -  cost : 0.62331086  -MSE  7.881189487164385  -Training accuracy : 0.7099057\n",
      "epoch:  353  -  cost : 0.6230885  -MSE  7.88376950424834  -Training accuracy : 0.7099057\n",
      "epoch:  354  -  cost : 0.6228662  -MSE  7.8863517660291365  -Training accuracy : 0.7099057\n",
      "epoch:  355  -  cost : 0.62264454  -MSE  7.888924478600972  -Training accuracy : 0.7099057\n",
      "epoch:  356  -  cost : 0.62242293  -MSE  7.891512347180154  -Training accuracy : 0.7099057\n",
      "epoch:  357  -  cost : 0.6222013  -MSE  7.894096578817054  -Training accuracy : 0.7099057\n",
      "epoch:  358  -  cost : 0.6219798  -MSE  7.896673824298546  -Training accuracy : 0.7099057\n",
      "epoch:  359  -  cost : 0.6217582  -MSE  7.899247009707257  -Training accuracy : 0.7110849\n",
      "epoch:  360  -  cost : 0.6215367  -MSE  7.901821590827737  -Training accuracy : 0.7110849\n",
      "epoch:  361  -  cost : 0.62131506  -MSE  7.904407316553999  -Training accuracy : 0.7110849\n",
      "epoch:  362  -  cost : 0.6210938  -MSE  7.907003691724315  -Training accuracy : 0.7122642\n",
      "epoch:  363  -  cost : 0.6208731  -MSE  7.909595454770686  -Training accuracy : 0.7122642\n",
      "epoch:  364  -  cost : 0.6206523  -MSE  7.912191454948308  -Training accuracy : 0.7122642\n",
      "epoch:  365  -  cost : 0.6204314  -MSE  7.914797523938491  -Training accuracy : 0.7122642\n",
      "epoch:  366  -  cost : 0.6202105  -MSE  7.917406641342047  -Training accuracy : 0.7122642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  367  -  cost : 0.6199897  -MSE  7.9200080734658815  -Training accuracy : 0.7122642\n",
      "epoch:  368  -  cost : 0.61976904  -MSE  7.922612676545086  -Training accuracy : 0.7122642\n",
      "epoch:  369  -  cost : 0.6195483  -MSE  7.9252021811373075  -Training accuracy : 0.7122642\n",
      "epoch:  370  -  cost : 0.6193275  -MSE  7.927790063926503  -Training accuracy : 0.7122642\n",
      "epoch:  371  -  cost : 0.61910653  -MSE  7.930395406056412  -Training accuracy : 0.7134434\n",
      "epoch:  372  -  cost : 0.6188855  -MSE  7.933009671577768  -Training accuracy : 0.7146226\n",
      "epoch:  373  -  cost : 0.61866474  -MSE  7.935628059332395  -Training accuracy : 0.7146226\n",
      "epoch:  374  -  cost : 0.61844367  -MSE  7.9382402207316725  -Training accuracy : 0.7146226\n",
      "epoch:  375  -  cost : 0.6182231  -MSE  7.940846297230695  -Training accuracy : 0.7146226\n",
      "epoch:  376  -  cost : 0.61800253  -MSE  7.9434588735727045  -Training accuracy : 0.7146226\n",
      "epoch:  377  -  cost : 0.61778176  -MSE  7.946077673478477  -Training accuracy : 0.7146226\n",
      "epoch:  378  -  cost : 0.6175606  -MSE  7.9487244721838834  -Training accuracy : 0.7146226\n",
      "epoch:  379  -  cost : 0.6173398  -MSE  7.951371143905583  -Training accuracy : 0.7146226\n",
      "epoch:  380  -  cost : 0.61711913  -MSE  7.954026760066322  -Training accuracy : 0.7146226\n",
      "epoch:  381  -  cost : 0.6168989  -MSE  7.9566831740074075  -Training accuracy : 0.7146226\n",
      "epoch:  382  -  cost : 0.61667883  -MSE  7.959338535725675  -Training accuracy : 0.7146226\n",
      "epoch:  383  -  cost : 0.61645895  -MSE  7.9619982358268455  -Training accuracy : 0.7146226\n",
      "epoch:  384  -  cost : 0.616239  -MSE  7.964663056999635  -Training accuracy : 0.7146226\n",
      "epoch:  385  -  cost : 0.61601907  -MSE  7.967333561040138  -Training accuracy : 0.7146226\n",
      "epoch:  386  -  cost : 0.6157993  -MSE  7.970001836431276  -Training accuracy : 0.7146226\n",
      "epoch:  387  -  cost : 0.6155794  -MSE  7.9726694376699  -Training accuracy : 0.7158019\n",
      "epoch:  388  -  cost : 0.6153603  -MSE  7.975326830905427  -Training accuracy : 0.7158019\n",
      "epoch:  389  -  cost : 0.6151415  -MSE  7.9780132798578265  -Training accuracy : 0.7158019\n",
      "epoch:  390  -  cost : 0.61492264  -MSE  7.980694227418203  -Training accuracy : 0.7158019\n",
      "epoch:  391  -  cost : 0.61470366  -MSE  7.9833763460502025  -Training accuracy : 0.7158019\n",
      "epoch:  392  -  cost : 0.61448455  -MSE  7.986065305482482  -Training accuracy : 0.7158019\n",
      "epoch:  393  -  cost : 0.6142653  -MSE  7.9887531208606575  -Training accuracy : 0.7158019\n",
      "epoch:  394  -  cost : 0.6140462  -MSE  7.991439219290673  -Training accuracy : 0.7158019\n",
      "epoch:  395  -  cost : 0.6138271  -MSE  7.994124490481063  -Training accuracy : 0.7158019\n",
      "epoch:  396  -  cost : 0.6136079  -MSE  7.996806414528603  -Training accuracy : 0.7158019\n",
      "epoch:  397  -  cost : 0.61338884  -MSE  7.9994895443625165  -Training accuracy : 0.7158019\n",
      "epoch:  398  -  cost : 0.61316997  -MSE  8.002173212174782  -Training accuracy : 0.7158019\n",
      "epoch:  399  -  cost : 0.61295134  -MSE  8.004866585323665  -Training accuracy : 0.7158019\n",
      "epoch:  400  -  cost : 0.61273265  -MSE  8.007550295274902  -Training accuracy : 0.7158019\n",
      "epoch:  401  -  cost : 0.61251426  -MSE  8.010225586858274  -Training accuracy : 0.7158019\n",
      "epoch:  402  -  cost : 0.6122956  -MSE  8.012900892153816  -Training accuracy : 0.7158019\n",
      "epoch:  403  -  cost : 0.61207664  -MSE  8.015583558376496  -Training accuracy : 0.7158019\n",
      "epoch:  404  -  cost : 0.611858  -MSE  8.01826108754538  -Training accuracy : 0.7158019\n",
      "epoch:  405  -  cost : 0.6116399  -MSE  8.020934664561938  -Training accuracy : 0.7158019\n",
      "epoch:  406  -  cost : 0.6114218  -MSE  8.023604994860403  -Training accuracy : 0.7158019\n",
      "epoch:  407  -  cost : 0.611204  -MSE  8.02627243336472  -Training accuracy : 0.7158019\n",
      "epoch:  408  -  cost : 0.6109864  -MSE  8.028931849431295  -Training accuracy : 0.7158019\n",
      "epoch:  409  -  cost : 0.61076844  -MSE  8.031605601191579  -Training accuracy : 0.7158019\n",
      "epoch:  410  -  cost : 0.61055064  -MSE  8.034270851836803  -Training accuracy : 0.7158019\n",
      "epoch:  411  -  cost : 0.61033285  -MSE  8.036923735906496  -Training accuracy : 0.7158019\n",
      "epoch:  412  -  cost : 0.61011505  -MSE  8.039579312716928  -Training accuracy : 0.7158019\n",
      "epoch:  413  -  cost : 0.6098974  -MSE  8.042243951255282  -Training accuracy : 0.7158019\n",
      "epoch:  414  -  cost : 0.60967994  -MSE  8.044906113601487  -Training accuracy : 0.7158019\n",
      "epoch:  415  -  cost : 0.6094627  -MSE  8.04756549700751  -Training accuracy : 0.7158019\n",
      "epoch:  416  -  cost : 0.6092456  -MSE  8.05022088934832  -Training accuracy : 0.7158019\n",
      "epoch:  417  -  cost : 0.6090287  -MSE  8.052874128386527  -Training accuracy : 0.7158019\n",
      "epoch:  418  -  cost : 0.60881156  -MSE  8.055533557663805  -Training accuracy : 0.7158019\n",
      "epoch:  419  -  cost : 0.60859436  -MSE  8.058192394342807  -Training accuracy : 0.7158019\n",
      "epoch:  420  -  cost : 0.60837716  -MSE  8.060855122766247  -Training accuracy : 0.7158019\n",
      "epoch:  421  -  cost : 0.6081598  -MSE  8.063520732614005  -Training accuracy : 0.7158019\n",
      "epoch:  422  -  cost : 0.60794216  -MSE  8.066186464623975  -Training accuracy : 0.7158019\n",
      "epoch:  423  -  cost : 0.6077247  -MSE  8.068847374362099  -Training accuracy : 0.7169811\n",
      "epoch:  424  -  cost : 0.60750747  -MSE  8.071505164714823  -Training accuracy : 0.7158019\n",
      "epoch:  425  -  cost : 0.6072903  -MSE  8.074160076670546  -Training accuracy : 0.7158019\n",
      "epoch:  426  -  cost : 0.60707307  -MSE  8.076817375674153  -Training accuracy : 0.7158019\n",
      "epoch:  427  -  cost : 0.6068559  -MSE  8.079472908753692  -Training accuracy : 0.7158019\n",
      "epoch:  428  -  cost : 0.6066391  -MSE  8.082124502127888  -Training accuracy : 0.7158019\n",
      "epoch:  429  -  cost : 0.6064223  -MSE  8.08478605535134  -Training accuracy : 0.7169811\n",
      "epoch:  430  -  cost : 0.60620564  -MSE  8.087457649322612  -Training accuracy : 0.7169811\n",
      "epoch:  431  -  cost : 0.60598904  -MSE  8.09012550265752  -Training accuracy : 0.7169811\n",
      "epoch:  432  -  cost : 0.6057726  -MSE  8.09278751917415  -Training accuracy : 0.7169811\n",
      "epoch:  433  -  cost : 0.6055562  -MSE  8.095451321050673  -Training accuracy : 0.7169811\n",
      "epoch:  434  -  cost : 0.6053395  -MSE  8.09810778927631  -Training accuracy : 0.7169811\n",
      "epoch:  435  -  cost : 0.605123  -MSE  8.100766958869075  -Training accuracy : 0.7169811\n",
      "epoch:  436  -  cost : 0.6049066  -MSE  8.103423782386276  -Training accuracy : 0.7169811\n",
      "epoch:  437  -  cost : 0.6046909  -MSE  8.106105392981792  -Training accuracy : 0.7181604\n",
      "epoch:  438  -  cost : 0.6044755  -MSE  8.108781231407267  -Training accuracy : 0.7181604\n",
      "epoch:  439  -  cost : 0.60426027  -MSE  8.11147695708661  -Training accuracy : 0.7193396\n",
      "epoch:  440  -  cost : 0.6040452  -MSE  8.11417224524323  -Training accuracy : 0.7193396\n",
      "epoch:  441  -  cost : 0.60383016  -MSE  8.116860315131698  -Training accuracy : 0.7193396\n",
      "epoch:  442  -  cost : 0.6036149  -MSE  8.119547478759717  -Training accuracy : 0.7193396\n",
      "epoch:  443  -  cost : 0.6033997  -MSE  8.122233408767086  -Training accuracy : 0.7193396\n",
      "epoch:  444  -  cost : 0.6031846  -MSE  8.124913451759108  -Training accuracy : 0.7193396\n",
      "epoch:  445  -  cost : 0.6029695  -MSE  8.127586115563942  -Training accuracy : 0.7193396\n",
      "epoch:  446  -  cost : 0.60275435  -MSE  8.130260130971134  -Training accuracy : 0.7205189\n",
      "epoch:  447  -  cost : 0.60253906  -MSE  8.132927309165275  -Training accuracy : 0.7205189\n",
      "epoch:  448  -  cost : 0.60232365  -MSE  8.1356030761609  -Training accuracy : 0.7205189\n",
      "epoch:  449  -  cost : 0.60210824  -MSE  8.138280336063964  -Training accuracy : 0.7205189\n",
      "epoch:  450  -  cost : 0.60189277  -MSE  8.140951857223078  -Training accuracy : 0.7205189\n",
      "epoch:  451  -  cost : 0.6016776  -MSE  8.143614123502923  -Training accuracy : 0.7216981\n",
      "epoch:  452  -  cost : 0.60146224  -MSE  8.146273064594018  -Training accuracy : 0.7216981\n",
      "epoch:  453  -  cost : 0.60124713  -MSE  8.148927046302305  -Training accuracy : 0.7216981\n",
      "epoch:  454  -  cost : 0.6010318  -MSE  8.151577027004146  -Training accuracy : 0.7216981\n",
      "epoch:  455  -  cost : 0.6008163  -MSE  8.154229788042855  -Training accuracy : 0.7216981\n",
      "epoch:  456  -  cost : 0.6006006  -MSE  8.156885768311298  -Training accuracy : 0.7216981\n",
      "epoch:  457  -  cost : 0.6003849  -MSE  8.159544190910287  -Training accuracy : 0.7216981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  458  -  cost : 0.6001692  -MSE  8.162205355938658  -Training accuracy : 0.7216981\n",
      "epoch:  459  -  cost : 0.59995353  -MSE  8.164863318476131  -Training accuracy : 0.7216981\n",
      "epoch:  460  -  cost : 0.599738  -MSE  8.167512806826641  -Training accuracy : 0.7228774\n",
      "epoch:  461  -  cost : 0.5995224  -MSE  8.170163286239504  -Training accuracy : 0.7228774\n",
      "epoch:  462  -  cost : 0.59930676  -MSE  8.17280874572487  -Training accuracy : 0.7228774\n",
      "epoch:  463  -  cost : 0.5990913  -MSE  8.175445900573013  -Training accuracy : 0.7228774\n",
      "epoch:  464  -  cost : 0.5988757  -MSE  8.178075479722944  -Training accuracy : 0.7228774\n",
      "epoch:  465  -  cost : 0.59865975  -MSE  8.180706302888936  -Training accuracy : 0.7228774\n",
      "epoch:  466  -  cost : 0.5984437  -MSE  8.18333367368575  -Training accuracy : 0.7228774\n",
      "epoch:  467  -  cost : 0.5982272  -MSE  8.18596161982832  -Training accuracy : 0.7228774\n",
      "epoch:  468  -  cost : 0.5980104  -MSE  8.188598568263478  -Training accuracy : 0.7228774\n",
      "epoch:  469  -  cost : 0.5977938  -MSE  8.191230240159692  -Training accuracy : 0.7228774\n",
      "epoch:  470  -  cost : 0.5975772  -MSE  8.193860817692084  -Training accuracy : 0.7228774\n",
      "epoch:  471  -  cost : 0.5973607  -MSE  8.196488740294015  -Training accuracy : 0.7240566\n",
      "epoch:  472  -  cost : 0.5971441  -MSE  8.19911507185589  -Training accuracy : 0.7240566\n",
      "epoch:  473  -  cost : 0.5969276  -MSE  8.201738111689657  -Training accuracy : 0.7252358\n",
      "epoch:  474  -  cost : 0.59671104  -MSE  8.204359312877651  -Training accuracy : 0.7252358\n",
      "epoch:  475  -  cost : 0.59649456  -MSE  8.2069579321286  -Training accuracy : 0.7252358\n",
      "epoch:  476  -  cost : 0.59627783  -MSE  8.209561057751953  -Training accuracy : 0.7264151\n",
      "epoch:  477  -  cost : 0.59606105  -MSE  8.212163528825151  -Training accuracy : 0.7275943\n",
      "epoch:  478  -  cost : 0.59584427  -MSE  8.214769918201146  -Training accuracy : 0.7287736\n",
      "epoch:  479  -  cost : 0.5956274  -MSE  8.217372291025365  -Training accuracy : 0.7287736\n",
      "epoch:  480  -  cost : 0.5954105  -MSE  8.219978459931435  -Training accuracy : 0.7299528\n",
      "epoch:  481  -  cost : 0.59519374  -MSE  8.222578088179622  -Training accuracy : 0.7299528\n",
      "epoch:  482  -  cost : 0.5949772  -MSE  8.22520904679071  -Training accuracy : 0.7311321\n",
      "epoch:  483  -  cost : 0.59476066  -MSE  8.227835604676331  -Training accuracy : 0.7311321\n",
      "epoch:  484  -  cost : 0.59454393  -MSE  8.230460256724461  -Training accuracy : 0.7311321\n",
      "epoch:  485  -  cost : 0.5943274  -MSE  8.233080441150696  -Training accuracy : 0.7311321\n",
      "epoch:  486  -  cost : 0.59411037  -MSE  8.235691791067689  -Training accuracy : 0.7311321\n",
      "epoch:  487  -  cost : 0.59389335  -MSE  8.23830182236846  -Training accuracy : 0.7311321\n",
      "epoch:  488  -  cost : 0.59367603  -MSE  8.240912533406599  -Training accuracy : 0.7311321\n",
      "epoch:  489  -  cost : 0.5934585  -MSE  8.243517994829782  -Training accuracy : 0.7311321\n",
      "epoch:  490  -  cost : 0.5932412  -MSE  8.246119419027545  -Training accuracy : 0.7311321\n",
      "epoch:  491  -  cost : 0.59302396  -MSE  8.248722545548011  -Training accuracy : 0.7311321\n",
      "epoch:  492  -  cost : 0.5928065  -MSE  8.251315851535834  -Training accuracy : 0.7311321\n",
      "epoch:  493  -  cost : 0.5925892  -MSE  8.253912116053545  -Training accuracy : 0.7311321\n",
      "epoch:  494  -  cost : 0.59237176  -MSE  8.25650391280849  -Training accuracy : 0.7311321\n",
      "epoch:  495  -  cost : 0.59215444  -MSE  8.259093036439143  -Training accuracy : 0.7311321\n",
      "epoch:  496  -  cost : 0.59193724  -MSE  8.261671305238275  -Training accuracy : 0.7323113\n",
      "epoch:  497  -  cost : 0.59172  -MSE  8.264237246264575  -Training accuracy : 0.7323113\n",
      "epoch:  498  -  cost : 0.59150267  -MSE  8.266802549151757  -Training accuracy : 0.7323113\n",
      "epoch:  499  -  cost : 0.5912852  -MSE  8.269356119112643  -Training accuracy : 0.7323113\n",
      "epoch:  500  -  cost : 0.591068  -MSE  8.27190804010159  -Training accuracy : 0.7323113\n",
      "epoch:  501  -  cost : 0.590851  -MSE  8.274477901370798  -Training accuracy : 0.7323113\n",
      "epoch:  502  -  cost : 0.5906339  -MSE  8.277032319383782  -Training accuracy : 0.7323113\n",
      "epoch:  503  -  cost : 0.5904165  -MSE  8.279587040182625  -Training accuracy : 0.7323113\n",
      "epoch:  504  -  cost : 0.590199  -MSE  8.282136807700452  -Training accuracy : 0.7323113\n",
      "epoch:  505  -  cost : 0.5899815  -MSE  8.284681732239012  -Training accuracy : 0.7323113\n",
      "epoch:  506  -  cost : 0.58976406  -MSE  8.287222078615967  -Training accuracy : 0.7323113\n",
      "epoch:  507  -  cost : 0.5895463  -MSE  8.289765137827922  -Training accuracy : 0.7323113\n",
      "epoch:  508  -  cost : 0.58932865  -MSE  8.292301261970762  -Training accuracy : 0.7323113\n",
      "epoch:  509  -  cost : 0.58911085  -MSE  8.294834621870224  -Training accuracy : 0.7323113\n",
      "epoch:  510  -  cost : 0.5888929  -MSE  8.29736210490665  -Training accuracy : 0.7323113\n",
      "epoch:  511  -  cost : 0.5886748  -MSE  8.299889545961697  -Training accuracy : 0.7323113\n",
      "epoch:  512  -  cost : 0.5884567  -MSE  8.302412023599317  -Training accuracy : 0.7323113\n",
      "epoch:  513  -  cost : 0.58823836  -MSE  8.304939805895192  -Training accuracy : 0.7323113\n",
      "epoch:  514  -  cost : 0.58801997  -MSE  8.307464762731344  -Training accuracy : 0.7323113\n",
      "epoch:  515  -  cost : 0.58780146  -MSE  8.30998187113644  -Training accuracy : 0.7334906\n",
      "epoch:  516  -  cost : 0.5875828  -MSE  8.312491282265432  -Training accuracy : 0.7334906\n",
      "epoch:  517  -  cost : 0.58736414  -MSE  8.314994691756862  -Training accuracy : 0.7334906\n",
      "epoch:  518  -  cost : 0.58714545  -MSE  8.317498893861313  -Training accuracy : 0.7334906\n",
      "epoch:  519  -  cost : 0.58692664  -MSE  8.319992415454536  -Training accuracy : 0.7334906\n",
      "epoch:  520  -  cost : 0.58670783  -MSE  8.32248537951047  -Training accuracy : 0.7334906\n",
      "epoch:  521  -  cost : 0.5864889  -MSE  8.32497834966005  -Training accuracy : 0.7323113\n",
      "epoch:  522  -  cost : 0.5862699  -MSE  8.327456564501478  -Training accuracy : 0.7323113\n",
      "epoch:  523  -  cost : 0.58605087  -MSE  8.329938180493713  -Training accuracy : 0.7334906\n",
      "epoch:  524  -  cost : 0.58583164  -MSE  8.332414117748057  -Training accuracy : 0.7334906\n",
      "epoch:  525  -  cost : 0.58561236  -MSE  8.334874408918447  -Training accuracy : 0.7334906\n",
      "epoch:  526  -  cost : 0.58539295  -MSE  8.337334093162971  -Training accuracy : 0.7334906\n",
      "epoch:  527  -  cost : 0.5851735  -MSE  8.339780356304853  -Training accuracy : 0.7334906\n",
      "epoch:  528  -  cost : 0.58495367  -MSE  8.342230269750504  -Training accuracy : 0.7334906\n",
      "epoch:  529  -  cost : 0.5847338  -MSE  8.344676330094734  -Training accuracy : 0.7334906\n",
      "epoch:  530  -  cost : 0.5845138  -MSE  8.34711394377234  -Training accuracy : 0.7334906\n",
      "epoch:  531  -  cost : 0.58429366  -MSE  8.349548149618155  -Training accuracy : 0.7334906\n",
      "epoch:  532  -  cost : 0.58407336  -MSE  8.351978705592167  -Training accuracy : 0.7334906\n",
      "epoch:  533  -  cost : 0.5838528  -MSE  8.354405054430341  -Training accuracy : 0.7334906\n",
      "epoch:  534  -  cost : 0.5836322  -MSE  8.356829742730882  -Training accuracy : 0.7334906\n",
      "epoch:  535  -  cost : 0.5834115  -MSE  8.359243769184912  -Training accuracy : 0.7334906\n",
      "epoch:  536  -  cost : 0.5831906  -MSE  8.361657068924355  -Training accuracy : 0.7334906\n",
      "epoch:  537  -  cost : 0.5829696  -MSE  8.364052018386166  -Training accuracy : 0.7334906\n",
      "epoch:  538  -  cost : 0.5827485  -MSE  8.366431080205539  -Training accuracy : 0.7334906\n",
      "epoch:  539  -  cost : 0.5825274  -MSE  8.368805785177505  -Training accuracy : 0.7334906\n",
      "epoch:  540  -  cost : 0.582306  -MSE  8.371177115576103  -Training accuracy : 0.7334906\n",
      "epoch:  541  -  cost : 0.5820847  -MSE  8.37353984651537  -Training accuracy : 0.7334906\n",
      "epoch:  542  -  cost : 0.5818631  -MSE  8.375900317173704  -Training accuracy : 0.7334906\n",
      "epoch:  543  -  cost : 0.5816414  -MSE  8.378255485874678  -Training accuracy : 0.7334906\n",
      "epoch:  544  -  cost : 0.58141947  -MSE  8.380603267130734  -Training accuracy : 0.7334906\n",
      "epoch:  545  -  cost : 0.58119744  -MSE  8.382953130356437  -Training accuracy : 0.7334906\n",
      "epoch:  546  -  cost : 0.5809752  -MSE  8.385290358932595  -Training accuracy : 0.7334906\n",
      "epoch:  547  -  cost : 0.58075285  -MSE  8.387629507476516  -Training accuracy : 0.7334906\n",
      "epoch:  548  -  cost : 0.5805303  -MSE  8.389965599564857  -Training accuracy : 0.7334906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  549  -  cost : 0.58030754  -MSE  8.392293122237998  -Training accuracy : 0.7334906\n",
      "epoch:  550  -  cost : 0.58008456  -MSE  8.394620294417757  -Training accuracy : 0.7334906\n",
      "epoch:  551  -  cost : 0.5798614  -MSE  8.396938572387267  -Training accuracy : 0.7334906\n",
      "epoch:  552  -  cost : 0.57963806  -MSE  8.39923888788804  -Training accuracy : 0.7334906\n",
      "epoch:  553  -  cost : 0.5794146  -MSE  8.40153538771075  -Training accuracy : 0.7334906\n",
      "epoch:  554  -  cost : 0.5791911  -MSE  8.403823004774516  -Training accuracy : 0.7334906\n",
      "epoch:  555  -  cost : 0.5789674  -MSE  8.406088931030567  -Training accuracy : 0.7334906\n",
      "epoch:  556  -  cost : 0.5787437  -MSE  8.408353957813212  -Training accuracy : 0.7334906\n",
      "epoch:  557  -  cost : 0.5785197  -MSE  8.410609303695818  -Training accuracy : 0.7334906\n",
      "epoch:  558  -  cost : 0.5782955  -MSE  8.412861650220524  -Training accuracy : 0.7334906\n",
      "epoch:  559  -  cost : 0.5780711  -MSE  8.415115842318885  -Training accuracy : 0.7334906\n",
      "epoch:  560  -  cost : 0.5778464  -MSE  8.417365071827435  -Training accuracy : 0.7346698\n",
      "epoch:  561  -  cost : 0.57762146  -MSE  8.419609599067039  -Training accuracy : 0.7346698\n",
      "epoch:  562  -  cost : 0.5773962  -MSE  8.421861192115204  -Training accuracy : 0.7346698\n",
      "epoch:  563  -  cost : 0.57717097  -MSE  8.42410251774248  -Training accuracy : 0.7346698\n",
      "epoch:  564  -  cost : 0.5769456  -MSE  8.426344867769759  -Training accuracy : 0.7346698\n",
      "epoch:  565  -  cost : 0.57671994  -MSE  8.428586199128862  -Training accuracy : 0.7346698\n",
      "epoch:  566  -  cost : 0.57649404  -MSE  8.430819569148285  -Training accuracy : 0.7346698\n",
      "epoch:  567  -  cost : 0.576268  -MSE  8.43305431584497  -Training accuracy : 0.7346698\n",
      "epoch:  568  -  cost : 0.57604176  -MSE  8.435278844993414  -Training accuracy : 0.7346698\n",
      "epoch:  569  -  cost : 0.5758155  -MSE  8.437499301829362  -Training accuracy : 0.7346698\n",
      "epoch:  570  -  cost : 0.5755892  -MSE  8.439745344865575  -Training accuracy : 0.7346698\n",
      "epoch:  571  -  cost : 0.57536274  -MSE  8.441982898422472  -Training accuracy : 0.7346698\n",
      "epoch:  572  -  cost : 0.575136  -MSE  8.444217038483155  -Training accuracy : 0.7346698\n",
      "epoch:  573  -  cost : 0.5749091  -MSE  8.44644313256189  -Training accuracy : 0.7346698\n",
      "epoch:  574  -  cost : 0.57468206  -MSE  8.44866135516571  -Training accuracy : 0.7358491\n",
      "epoch:  575  -  cost : 0.5744547  -MSE  8.45087618414018  -Training accuracy : 0.7358491\n",
      "epoch:  576  -  cost : 0.5742272  -MSE  8.453082543091881  -Training accuracy : 0.7358491\n",
      "epoch:  577  -  cost : 0.5739996  -MSE  8.455278639255614  -Training accuracy : 0.7358491\n",
      "epoch:  578  -  cost : 0.5737717  -MSE  8.457475231248429  -Training accuracy : 0.7370283\n",
      "epoch:  579  -  cost : 0.57354367  -MSE  8.459664247640731  -Training accuracy : 0.7370283\n",
      "epoch:  580  -  cost : 0.57331544  -MSE  8.461846214886524  -Training accuracy : 0.7370283\n",
      "epoch:  581  -  cost : 0.5730869  -MSE  8.464029011525165  -Training accuracy : 0.7370283\n",
      "epoch:  582  -  cost : 0.5728584  -MSE  8.466201945472436  -Training accuracy : 0.7370283\n",
      "epoch:  583  -  cost : 0.5726296  -MSE  8.468364366735177  -Training accuracy : 0.7370283\n",
      "epoch:  584  -  cost : 0.57240045  -MSE  8.470524054911811  -Training accuracy : 0.7370283\n",
      "epoch:  585  -  cost : 0.572171  -MSE  8.472677943825108  -Training accuracy : 0.7370283\n",
      "epoch:  586  -  cost : 0.5719414  -MSE  8.474823807141115  -Training accuracy : 0.7370283\n",
      "epoch:  587  -  cost : 0.57171166  -MSE  8.476975055117705  -Training accuracy : 0.7370283\n",
      "epoch:  588  -  cost : 0.57148165  -MSE  8.47912133795958  -Training accuracy : 0.7370283\n",
      "epoch:  589  -  cost : 0.57125133  -MSE  8.481258204955052  -Training accuracy : 0.7370283\n",
      "epoch:  590  -  cost : 0.57102096  -MSE  8.483393512845058  -Training accuracy : 0.7370283\n",
      "epoch:  591  -  cost : 0.5707903  -MSE  8.485521477562886  -Training accuracy : 0.7370283\n",
      "epoch:  592  -  cost : 0.57055944  -MSE  8.487636649673627  -Training accuracy : 0.7370283\n",
      "epoch:  593  -  cost : 0.57032835  -MSE  8.489752779630946  -Training accuracy : 0.7370283\n",
      "epoch:  594  -  cost : 0.570097  -MSE  8.49187113837107  -Training accuracy : 0.7370283\n",
      "epoch:  595  -  cost : 0.56986547  -MSE  8.493982338227095  -Training accuracy : 0.7370283\n",
      "epoch:  596  -  cost : 0.5696336  -MSE  8.496082252227708  -Training accuracy : 0.7382075\n",
      "epoch:  597  -  cost : 0.5694016  -MSE  8.49818407115193  -Training accuracy : 0.7382075\n",
      "epoch:  598  -  cost : 0.56916934  -MSE  8.500282777399653  -Training accuracy : 0.7382075\n",
      "epoch:  599  -  cost : 0.5689367  -MSE  8.50238137398561  -Training accuracy : 0.7382075\n",
      "epoch:  600  -  cost : 0.56870383  -MSE  8.504465406454923  -Training accuracy : 0.7382075\n",
      "epoch:  601  -  cost : 0.5684709  -MSE  8.506545543566954  -Training accuracy : 0.7382075\n",
      "epoch:  602  -  cost : 0.5682377  -MSE  8.508622211844353  -Training accuracy : 0.7382075\n",
      "epoch:  603  -  cost : 0.56800413  -MSE  8.510689946151684  -Training accuracy : 0.7382075\n",
      "epoch:  604  -  cost : 0.5677701  -MSE  8.512743492492573  -Training accuracy : 0.7382075\n",
      "epoch:  605  -  cost : 0.567536  -MSE  8.514796381458194  -Training accuracy : 0.7382075\n",
      "epoch:  606  -  cost : 0.5673014  -MSE  8.516847851410748  -Training accuracy : 0.7382075\n",
      "epoch:  607  -  cost : 0.5670663  -MSE  8.518891654369801  -Training accuracy : 0.7393868\n",
      "epoch:  608  -  cost : 0.5668311  -MSE  8.520923270199559  -Training accuracy : 0.740566\n",
      "epoch:  609  -  cost : 0.5665954  -MSE  8.522952793244665  -Training accuracy : 0.740566\n",
      "epoch:  610  -  cost : 0.56635946  -MSE  8.524975655638125  -Training accuracy : 0.7417453\n",
      "epoch:  611  -  cost : 0.56612325  -MSE  8.527005177027222  -Training accuracy : 0.7417453\n",
      "epoch:  612  -  cost : 0.565887  -MSE  8.529036079511362  -Training accuracy : 0.7417453\n",
      "epoch:  613  -  cost : 0.56565034  -MSE  8.531052480979683  -Training accuracy : 0.7417453\n",
      "epoch:  614  -  cost : 0.56541365  -MSE  8.533069391800803  -Training accuracy : 0.7417453\n",
      "epoch:  615  -  cost : 0.56517667  -MSE  8.53506529086176  -Training accuracy : 0.7417453\n",
      "epoch:  616  -  cost : 0.56493956  -MSE  8.537061767130158  -Training accuracy : 0.7417453\n",
      "epoch:  617  -  cost : 0.5647022  -MSE  8.539053176078811  -Training accuracy : 0.7417453\n",
      "epoch:  618  -  cost : 0.56446457  -MSE  8.541047463829926  -Training accuracy : 0.7417453\n",
      "epoch:  619  -  cost : 0.56422675  -MSE  8.543039367797755  -Training accuracy : 0.7429245\n",
      "epoch:  620  -  cost : 0.56398875  -MSE  8.545017779916211  -Training accuracy : 0.7429245\n",
      "epoch:  621  -  cost : 0.56375045  -MSE  8.546987255893502  -Training accuracy : 0.7429245\n",
      "epoch:  622  -  cost : 0.563512  -MSE  8.548940366334316  -Training accuracy : 0.7429245\n",
      "epoch:  623  -  cost : 0.5632734  -MSE  8.550886372942596  -Training accuracy : 0.7429245\n",
      "epoch:  624  -  cost : 0.5630344  -MSE  8.552824188079462  -Training accuracy : 0.7429245\n",
      "epoch:  625  -  cost : 0.56279516  -MSE  8.554761134158522  -Training accuracy : 0.7429245\n",
      "epoch:  626  -  cost : 0.56255573  -MSE  8.556681875596249  -Training accuracy : 0.7429245\n",
      "epoch:  627  -  cost : 0.5623161  -MSE  8.558593975387451  -Training accuracy : 0.7429245\n",
      "epoch:  628  -  cost : 0.56207615  -MSE  8.560490919540992  -Training accuracy : 0.7429245\n",
      "epoch:  629  -  cost : 0.56183594  -MSE  8.562382703507504  -Training accuracy : 0.7441038\n",
      "epoch:  630  -  cost : 0.5615956  -MSE  8.56426474880325  -Training accuracy : 0.7464623\n",
      "epoch:  631  -  cost : 0.56135494  -MSE  8.56614266759148  -Training accuracy : 0.7476415\n",
      "epoch:  632  -  cost : 0.56111395  -MSE  8.56801328454395  -Training accuracy : 0.7476415\n",
      "epoch:  633  -  cost : 0.5608727  -MSE  8.56987858873734  -Training accuracy : 0.7476415\n",
      "epoch:  634  -  cost : 0.56063116  -MSE  8.571737802497603  -Training accuracy : 0.7476415\n",
      "epoch:  635  -  cost : 0.56038916  -MSE  8.57358869644082  -Training accuracy : 0.7476415\n",
      "epoch:  636  -  cost : 0.56014705  -MSE  8.57543446740404  -Training accuracy : 0.7476415\n",
      "epoch:  637  -  cost : 0.5599045  -MSE  8.57727495944267  -Training accuracy : 0.7476415\n",
      "epoch:  638  -  cost : 0.5596617  -MSE  8.579103998980676  -Training accuracy : 0.7476415\n",
      "epoch:  639  -  cost : 0.5594186  -MSE  8.580921644430056  -Training accuracy : 0.7476415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  640  -  cost : 0.55917525  -MSE  8.582734414515459  -Training accuracy : 0.7476415\n",
      "epoch:  641  -  cost : 0.5589315  -MSE  8.584544286642666  -Training accuracy : 0.7476415\n",
      "epoch:  642  -  cost : 0.55868745  -MSE  8.586344534956453  -Training accuracy : 0.7476415\n",
      "epoch:  643  -  cost : 0.5584431  -MSE  8.588145046704138  -Training accuracy : 0.7476415\n",
      "epoch:  644  -  cost : 0.5581983  -MSE  8.589941131072694  -Training accuracy : 0.7476415\n",
      "epoch:  645  -  cost : 0.5579533  -MSE  8.591733985874951  -Training accuracy : 0.7488208\n",
      "epoch:  646  -  cost : 0.557708  -MSE  8.593520870964674  -Training accuracy : 0.7488208\n",
      "epoch:  647  -  cost : 0.5574621  -MSE  8.595299102194529  -Training accuracy : 0.7511792\n",
      "epoch:  648  -  cost : 0.5572159  -MSE  8.597071041085247  -Training accuracy : 0.7523585\n",
      "epoch:  649  -  cost : 0.5569693  -MSE  8.598844105540708  -Training accuracy : 0.7523585\n",
      "epoch:  650  -  cost : 0.55672234  -MSE  8.600613895400032  -Training accuracy : 0.7523585\n",
      "epoch:  651  -  cost : 0.55647475  -MSE  8.602378601765661  -Training accuracy : 0.7523585\n",
      "epoch:  652  -  cost : 0.5562269  -MSE  8.604147287342764  -Training accuracy : 0.7523585\n",
      "epoch:  653  -  cost : 0.5559787  -MSE  8.605911351639259  -Training accuracy : 0.7523585\n",
      "epoch:  654  -  cost : 0.5557302  -MSE  8.607675207612653  -Training accuracy : 0.7523585\n",
      "epoch:  655  -  cost : 0.55548143  -MSE  8.609435525660135  -Training accuracy : 0.7535377\n",
      "epoch:  656  -  cost : 0.55523235  -MSE  8.611190313240442  -Training accuracy : 0.7535377\n",
      "epoch:  657  -  cost : 0.5549829  -MSE  8.612939511404946  -Training accuracy : 0.7558962\n",
      "epoch:  658  -  cost : 0.55473316  -MSE  8.61468046406959  -Training accuracy : 0.7558962\n",
      "epoch:  659  -  cost : 0.5544831  -MSE  8.616424123278707  -Training accuracy : 0.7558962\n",
      "epoch:  660  -  cost : 0.5542328  -MSE  8.61816428511593  -Training accuracy : 0.7558962\n",
      "epoch:  661  -  cost : 0.5539819  -MSE  8.61990256876899  -Training accuracy : 0.7558962\n",
      "epoch:  662  -  cost : 0.55373085  -MSE  8.621636855918009  -Training accuracy : 0.7558962\n",
      "epoch:  663  -  cost : 0.5534794  -MSE  8.623367617508654  -Training accuracy : 0.7558962\n",
      "epoch:  664  -  cost : 0.55322754  -MSE  8.625094115190327  -Training accuracy : 0.7558962\n",
      "epoch:  665  -  cost : 0.5529751  -MSE  8.626819071728777  -Training accuracy : 0.7558962\n",
      "epoch:  666  -  cost : 0.5527223  -MSE  8.628543410241617  -Training accuracy : 0.7558962\n",
      "epoch:  667  -  cost : 0.552469  -MSE  8.630265120638557  -Training accuracy : 0.7558962\n",
      "epoch:  668  -  cost : 0.55221546  -MSE  8.63198218969817  -Training accuracy : 0.7558962\n",
      "epoch:  669  -  cost : 0.5519616  -MSE  8.633698716409743  -Training accuracy : 0.7558962\n",
      "epoch:  670  -  cost : 0.55170745  -MSE  8.635422964092918  -Training accuracy : 0.7558962\n",
      "epoch:  671  -  cost : 0.55145276  -MSE  8.637144789485752  -Training accuracy : 0.7558962\n",
      "epoch:  672  -  cost : 0.551198  -MSE  8.638859228446664  -Training accuracy : 0.7558962\n",
      "epoch:  673  -  cost : 0.55094284  -MSE  8.64057206544884  -Training accuracy : 0.7558962\n",
      "epoch:  674  -  cost : 0.5506872  -MSE  8.642283037921642  -Training accuracy : 0.7558962\n",
      "epoch:  675  -  cost : 0.55043125  -MSE  8.643992635484436  -Training accuracy : 0.7558962\n",
      "epoch:  676  -  cost : 0.55017495  -MSE  8.645702443191  -Training accuracy : 0.7570755\n",
      "epoch:  677  -  cost : 0.54991835  -MSE  8.64740713661012  -Training accuracy : 0.7570755\n",
      "epoch:  678  -  cost : 0.5496613  -MSE  8.649104862453877  -Training accuracy : 0.7570755\n",
      "epoch:  679  -  cost : 0.5494039  -MSE  8.65080214136984  -Training accuracy : 0.7570755\n",
      "epoch:  680  -  cost : 0.54914606  -MSE  8.652497137723477  -Training accuracy : 0.7570755\n",
      "epoch:  681  -  cost : 0.5488877  -MSE  8.654183342052876  -Training accuracy : 0.7582547\n",
      "epoch:  682  -  cost : 0.5486291  -MSE  8.655864263702362  -Training accuracy : 0.759434\n",
      "epoch:  683  -  cost : 0.5483701  -MSE  8.657545239989917  -Training accuracy : 0.759434\n",
      "epoch:  684  -  cost : 0.5481107  -MSE  8.659229522162047  -Training accuracy : 0.759434\n",
      "epoch:  685  -  cost : 0.54785097  -MSE  8.660909513907995  -Training accuracy : 0.759434\n",
      "epoch:  686  -  cost : 0.54759103  -MSE  8.662591683520857  -Training accuracy : 0.759434\n",
      "epoch:  687  -  cost : 0.54733056  -MSE  8.664280632590362  -Training accuracy : 0.7606132\n",
      "epoch:  688  -  cost : 0.5470699  -MSE  8.665972688612632  -Training accuracy : 0.7617925\n",
      "epoch:  689  -  cost : 0.5468088  -MSE  8.667644743327296  -Training accuracy : 0.7617925\n",
      "epoch:  690  -  cost : 0.54654735  -MSE  8.669316467312786  -Training accuracy : 0.7617925\n",
      "epoch:  691  -  cost : 0.54628557  -MSE  8.670981367495292  -Training accuracy : 0.7617925\n",
      "epoch:  692  -  cost : 0.54602337  -MSE  8.672639073055375  -Training accuracy : 0.7617925\n",
      "epoch:  693  -  cost : 0.5457609  -MSE  8.674285219015228  -Training accuracy : 0.7617925\n",
      "epoch:  694  -  cost : 0.545498  -MSE  8.675925232537745  -Training accuracy : 0.7617925\n",
      "epoch:  695  -  cost : 0.54523474  -MSE  8.677560541202945  -Training accuracy : 0.7617925\n",
      "epoch:  696  -  cost : 0.5449711  -MSE  8.67919000928241  -Training accuracy : 0.7617925\n",
      "epoch:  697  -  cost : 0.54470706  -MSE  8.68081503354419  -Training accuracy : 0.7617925\n",
      "epoch:  698  -  cost : 0.5444427  -MSE  8.682434925407348  -Training accuracy : 0.7617925\n",
      "epoch:  699  -  cost : 0.5441779  -MSE  8.684049948054748  -Training accuracy : 0.7617925\n",
      "epoch:  700  -  cost : 0.5439127  -MSE  8.685660605499184  -Training accuracy : 0.7641509\n",
      "epoch:  701  -  cost : 0.5436471  -MSE  8.687261534769979  -Training accuracy : 0.7641509\n",
      "epoch:  702  -  cost : 0.5433812  -MSE  8.688858294893084  -Training accuracy : 0.7641509\n",
      "epoch:  703  -  cost : 0.54311484  -MSE  8.690450127820231  -Training accuracy : 0.7641509\n",
      "epoch:  704  -  cost : 0.54284817  -MSE  8.692038964582807  -Training accuracy : 0.7641509\n",
      "epoch:  705  -  cost : 0.542581  -MSE  8.693623977731553  -Training accuracy : 0.7641509\n",
      "epoch:  706  -  cost : 0.54231346  -MSE  8.695207035347417  -Training accuracy : 0.7641509\n",
      "epoch:  707  -  cost : 0.5420455  -MSE  8.696788087275522  -Training accuracy : 0.7641509\n",
      "epoch:  708  -  cost : 0.5417771  -MSE  8.698363675247158  -Training accuracy : 0.7641509\n",
      "epoch:  709  -  cost : 0.5415083  -MSE  8.699932837894876  -Training accuracy : 0.7641509\n",
      "epoch:  710  -  cost : 0.5412391  -MSE  8.701515047977335  -Training accuracy : 0.7641509\n",
      "epoch:  711  -  cost : 0.54096943  -MSE  8.703080662957083  -Training accuracy : 0.7641509\n",
      "epoch:  712  -  cost : 0.5406996  -MSE  8.704640914166223  -Training accuracy : 0.7641509\n",
      "epoch:  713  -  cost : 0.5404292  -MSE  8.706199834235061  -Training accuracy : 0.7653302\n",
      "epoch:  714  -  cost : 0.54015833  -MSE  8.707753816593463  -Training accuracy : 0.7653302\n",
      "epoch:  715  -  cost : 0.53988725  -MSE  8.709299982057495  -Training accuracy : 0.7665094\n",
      "epoch:  716  -  cost : 0.53961563  -MSE  8.71084306336469  -Training accuracy : 0.7665094\n",
      "epoch:  717  -  cost : 0.5393436  -MSE  8.712384581507367  -Training accuracy : 0.7665094\n",
      "epoch:  718  -  cost : 0.53907114  -MSE  8.71392319108717  -Training accuracy : 0.7665094\n",
      "epoch:  719  -  cost : 0.53879833  -MSE  8.715458836040638  -Training accuracy : 0.7665094\n",
      "epoch:  720  -  cost : 0.5385251  -MSE  8.716993061117629  -Training accuracy : 0.7676887\n",
      "epoch:  721  -  cost : 0.5382516  -MSE  8.718521956244828  -Training accuracy : 0.7676887\n",
      "epoch:  722  -  cost : 0.5379775  -MSE  8.720049430650086  -Training accuracy : 0.7676887\n",
      "epoch:  723  -  cost : 0.53770304  -MSE  8.72157266813236  -Training accuracy : 0.7676887\n",
      "epoch:  724  -  cost : 0.537428  -MSE  8.723100623598452  -Training accuracy : 0.7676887\n",
      "epoch:  725  -  cost : 0.5371526  -MSE  8.724618637892455  -Training accuracy : 0.7688679\n",
      "epoch:  726  -  cost : 0.53687674  -MSE  8.726136314484867  -Training accuracy : 0.7688679\n",
      "epoch:  727  -  cost : 0.53660053  -MSE  8.727649318208982  -Training accuracy : 0.7688679\n",
      "epoch:  728  -  cost : 0.53632385  -MSE  8.72916201854137  -Training accuracy : 0.7688679\n",
      "epoch:  729  -  cost : 0.5360465  -MSE  8.730667624356306  -Training accuracy : 0.7700472\n",
      "epoch:  730  -  cost : 0.5357689  -MSE  8.73217133892617  -Training accuracy : 0.7700472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  731  -  cost : 0.5354909  -MSE  8.733681658536316  -Training accuracy : 0.7700472\n",
      "epoch:  732  -  cost : 0.5352122  -MSE  8.735192937573181  -Training accuracy : 0.7700472\n",
      "epoch:  733  -  cost : 0.53493327  -MSE  8.736684588378072  -Training accuracy : 0.7712264\n",
      "epoch:  734  -  cost : 0.53465354  -MSE  8.738193470430227  -Training accuracy : 0.7712264\n",
      "epoch:  735  -  cost : 0.5343736  -MSE  8.73969290949868  -Training accuracy : 0.7724057\n",
      "epoch:  736  -  cost : 0.5340931  -MSE  8.74119209804408  -Training accuracy : 0.7724057\n",
      "epoch:  737  -  cost : 0.53381205  -MSE  8.742690650223517  -Training accuracy : 0.7724057\n",
      "epoch:  738  -  cost : 0.53353053  -MSE  8.744184944387074  -Training accuracy : 0.7724057\n",
      "epoch:  739  -  cost : 0.5332484  -MSE  8.745677241820351  -Training accuracy : 0.7735849\n",
      "epoch:  740  -  cost : 0.532966  -MSE  8.747174759116861  -Training accuracy : 0.7747642\n",
      "epoch:  741  -  cost : 0.5326832  -MSE  8.748669787129737  -Training accuracy : 0.7747642\n",
      "epoch:  742  -  cost : 0.5323999  -MSE  8.750160754943648  -Training accuracy : 0.7747642\n",
      "epoch:  743  -  cost : 0.53211606  -MSE  8.751650789150801  -Training accuracy : 0.7747642\n",
      "epoch:  744  -  cost : 0.531832  -MSE  8.753138017532201  -Training accuracy : 0.7747642\n",
      "epoch:  745  -  cost : 0.5315474  -MSE  8.754621509453672  -Training accuracy : 0.7747642\n",
      "epoch:  746  -  cost : 0.5312624  -MSE  8.756105406231061  -Training accuracy : 0.7747642\n",
      "epoch:  747  -  cost : 0.5309771  -MSE  8.757587244805137  -Training accuracy : 0.7747642\n",
      "epoch:  748  -  cost : 0.5306914  -MSE  8.759091630667813  -Training accuracy : 0.7747642\n",
      "epoch:  749  -  cost : 0.53040534  -MSE  8.76059950329426  -Training accuracy : 0.7747642\n",
      "epoch:  750  -  cost : 0.53011876  -MSE  8.762098129165917  -Training accuracy : 0.7747642\n",
      "epoch:  751  -  cost : 0.5298317  -MSE  8.763594385051551  -Training accuracy : 0.7747642\n",
      "epoch:  752  -  cost : 0.52954423  -MSE  8.765083419315143  -Training accuracy : 0.7783019\n",
      "epoch:  753  -  cost : 0.52925634  -MSE  8.766575566583171  -Training accuracy : 0.7783019\n",
      "epoch:  754  -  cost : 0.52896804  -MSE  8.768061319960369  -Training accuracy : 0.7783019\n",
      "epoch:  755  -  cost : 0.52867925  -MSE  8.76954951875045  -Training accuracy : 0.7783019\n",
      "epoch:  756  -  cost : 0.5283901  -MSE  8.771037620043414  -Training accuracy : 0.7794811\n",
      "epoch:  757  -  cost : 0.52810055  -MSE  8.77251917055049  -Training accuracy : 0.7794811\n",
      "epoch:  758  -  cost : 0.52781045  -MSE  8.773992198722329  -Training accuracy : 0.7806604\n",
      "epoch:  759  -  cost : 0.52751994  -MSE  8.775468284254886  -Training accuracy : 0.7806604\n",
      "epoch:  760  -  cost : 0.5272289  -MSE  8.776933821134575  -Training accuracy : 0.7806604\n",
      "epoch:  761  -  cost : 0.5269374  -MSE  8.778401804355898  -Training accuracy : 0.7818396\n",
      "epoch:  762  -  cost : 0.5266456  -MSE  8.779864868346618  -Training accuracy : 0.7818396\n",
      "epoch:  763  -  cost : 0.5263533  -MSE  8.781320138503782  -Training accuracy : 0.7818396\n",
      "epoch:  764  -  cost : 0.5260606  -MSE  8.782773892123386  -Training accuracy : 0.7818396\n",
      "epoch:  765  -  cost : 0.52576745  -MSE  8.784221253860956  -Training accuracy : 0.7818396\n",
      "epoch:  766  -  cost : 0.52547365  -MSE  8.785667042837694  -Training accuracy : 0.7818396\n",
      "epoch:  767  -  cost : 0.5251795  -MSE  8.787108799714636  -Training accuracy : 0.7818396\n",
      "epoch:  768  -  cost : 0.5248849  -MSE  8.788545226934142  -Training accuracy : 0.7830189\n",
      "epoch:  769  -  cost : 0.52458984  -MSE  8.789980355383593  -Training accuracy : 0.7830189\n",
      "epoch:  770  -  cost : 0.5242942  -MSE  8.791412316741521  -Training accuracy : 0.7830189\n",
      "epoch:  771  -  cost : 0.52399826  -MSE  8.79284001677455  -Training accuracy : 0.7841981\n",
      "epoch:  772  -  cost : 0.5237018  -MSE  8.794267728195903  -Training accuracy : 0.7841981\n",
      "epoch:  773  -  cost : 0.52340484  -MSE  8.795690590538856  -Training accuracy : 0.7853774\n",
      "epoch:  774  -  cost : 0.52310747  -MSE  8.797108918599948  -Training accuracy : 0.7865566\n",
      "epoch:  775  -  cost : 0.52280957  -MSE  8.798524774878825  -Training accuracy : 0.7865566\n",
      "epoch:  776  -  cost : 0.5225112  -MSE  8.799933958429735  -Training accuracy : 0.7865566\n",
      "epoch:  777  -  cost : 0.5222124  -MSE  8.801340858901815  -Training accuracy : 0.7865566\n",
      "epoch:  778  -  cost : 0.5219131  -MSE  8.802743760951312  -Training accuracy : 0.7865566\n",
      "epoch:  779  -  cost : 0.52161336  -MSE  8.804143778962572  -Training accuracy : 0.7865566\n",
      "epoch:  780  -  cost : 0.52131313  -MSE  8.805541072854666  -Training accuracy : 0.7865566\n",
      "epoch:  781  -  cost : 0.5210124  -MSE  8.806938115615495  -Training accuracy : 0.7877358\n",
      "epoch:  782  -  cost : 0.52071136  -MSE  8.80833025930373  -Training accuracy : 0.7877358\n",
      "epoch:  783  -  cost : 0.5204097  -MSE  8.809715320948282  -Training accuracy : 0.7877358\n",
      "epoch:  784  -  cost : 0.5201077  -MSE  8.811098579072963  -Training accuracy : 0.7889151\n",
      "epoch:  785  -  cost : 0.5198051  -MSE  8.812479163471997  -Training accuracy : 0.7889151\n",
      "epoch:  786  -  cost : 0.519502  -MSE  8.813852518987071  -Training accuracy : 0.7889151\n",
      "epoch:  787  -  cost : 0.51919836  -MSE  8.815224368692729  -Training accuracy : 0.7889151\n",
      "epoch:  788  -  cost : 0.51889443  -MSE  8.816589522284701  -Training accuracy : 0.7889151\n",
      "epoch:  789  -  cost : 0.51858985  -MSE  8.817951732906135  -Training accuracy : 0.7900943\n",
      "epoch:  790  -  cost : 0.5182848  -MSE  8.819308184064855  -Training accuracy : 0.7900943\n",
      "epoch:  791  -  cost : 0.5179793  -MSE  8.820663571108321  -Training accuracy : 0.7900943\n",
      "epoch:  792  -  cost : 0.51767325  -MSE  8.822015682239178  -Training accuracy : 0.7900943\n",
      "epoch:  793  -  cost : 0.5173668  -MSE  8.823363185536099  -Training accuracy : 0.7900943\n",
      "epoch:  794  -  cost : 0.5170599  -MSE  8.824709299078838  -Training accuracy : 0.7900943\n",
      "epoch:  795  -  cost : 0.51675236  -MSE  8.826050010590995  -Training accuracy : 0.7912736\n",
      "epoch:  796  -  cost : 0.5164444  -MSE  8.827391221840927  -Training accuracy : 0.7912736\n",
      "epoch:  797  -  cost : 0.5161359  -MSE  8.828726852057112  -Training accuracy : 0.7912736\n",
      "epoch:  798  -  cost : 0.5158269  -MSE  8.830062144273397  -Training accuracy : 0.7924528\n",
      "epoch:  799  -  cost : 0.51551753  -MSE  8.831397259825831  -Training accuracy : 0.7924528\n",
      "epoch:  800  -  cost : 0.5152075  -MSE  8.832728201730779  -Training accuracy : 0.7924528\n",
      "epoch:  801  -  cost : 0.51489705  -MSE  8.834052696159054  -Training accuracy : 0.7936321\n",
      "epoch:  802  -  cost : 0.51458615  -MSE  8.835375008622213  -Training accuracy : 0.7936321\n",
      "epoch:  803  -  cost : 0.51427466  -MSE  8.836687075361372  -Training accuracy : 0.7936321\n",
      "epoch:  804  -  cost : 0.51396245  -MSE  8.837996535506155  -Training accuracy : 0.7936321\n",
      "epoch:  805  -  cost : 0.5136498  -MSE  8.839301433053786  -Training accuracy : 0.7936321\n",
      "epoch:  806  -  cost : 0.5133366  -MSE  8.840605639159506  -Training accuracy : 0.7948113\n",
      "epoch:  807  -  cost : 0.5130229  -MSE  8.841909071877794  -Training accuracy : 0.7959906\n",
      "epoch:  808  -  cost : 0.5127087  -MSE  8.843210274567847  -Training accuracy : 0.7959906\n",
      "epoch:  809  -  cost : 0.512394  -MSE  8.844509203484808  -Training accuracy : 0.7959906\n",
      "epoch:  810  -  cost : 0.5120787  -MSE  8.845808883279206  -Training accuracy : 0.7959906\n",
      "epoch:  811  -  cost : 0.51176304  -MSE  8.847105621485328  -Training accuracy : 0.7959906\n",
      "epoch:  812  -  cost : 0.5114468  -MSE  8.848401450843765  -Training accuracy : 0.7959906\n",
      "epoch:  813  -  cost : 0.51113  -MSE  8.849697354930278  -Training accuracy : 0.7959906\n",
      "epoch:  814  -  cost : 0.51081264  -MSE  8.850989642986521  -Training accuracy : 0.7959906\n",
      "epoch:  815  -  cost : 0.5104949  -MSE  8.852280573980456  -Training accuracy : 0.7959906\n",
      "epoch:  816  -  cost : 0.51017654  -MSE  8.853569722045092  -Training accuracy : 0.7971698\n",
      "epoch:  817  -  cost : 0.50985765  -MSE  8.854859036379956  -Training accuracy : 0.7971698\n",
      "epoch:  818  -  cost : 0.50953823  -MSE  8.85614427875992  -Training accuracy : 0.7971698\n",
      "epoch:  819  -  cost : 0.5092184  -MSE  8.857427124595855  -Training accuracy : 0.7971698\n",
      "epoch:  820  -  cost : 0.50889796  -MSE  8.858708126875039  -Training accuracy : 0.7971698\n",
      "epoch:  821  -  cost : 0.5085769  -MSE  8.859988143173283  -Training accuracy : 0.7971698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  822  -  cost : 0.5082556  -MSE  8.861266668749995  -Training accuracy : 0.7983491\n",
      "epoch:  823  -  cost : 0.50793344  -MSE  8.862542612675243  -Training accuracy : 0.7983491\n",
      "epoch:  824  -  cost : 0.5076106  -MSE  8.863813574989265  -Training accuracy : 0.7983491\n",
      "epoch:  825  -  cost : 0.5072873  -MSE  8.865084522545434  -Training accuracy : 0.7983491\n",
      "epoch:  826  -  cost : 0.5069636  -MSE  8.866354535323644  -Training accuracy : 0.7983491\n",
      "epoch:  827  -  cost : 0.5066398  -MSE  8.86764940398643  -Training accuracy : 0.7983491\n",
      "epoch:  828  -  cost : 0.5063157  -MSE  8.868976962357994  -Training accuracy : 0.7995283\n",
      "epoch:  829  -  cost : 0.50599104  -MSE  8.870302389861845  -Training accuracy : 0.7995283\n",
      "epoch:  830  -  cost : 0.5056659  -MSE  8.871621237289238  -Training accuracy : 0.8007075\n",
      "epoch:  831  -  cost : 0.5053402  -MSE  8.872934886251683  -Training accuracy : 0.8007075\n",
      "epoch:  832  -  cost : 0.505014  -MSE  8.874239691746785  -Training accuracy : 0.8007075\n",
      "epoch:  833  -  cost : 0.504687  -MSE  8.87553805954453  -Training accuracy : 0.8007075\n",
      "epoch:  834  -  cost : 0.50435954  -MSE  8.87683882019262  -Training accuracy : 0.8007075\n",
      "epoch:  835  -  cost : 0.50403166  -MSE  8.87814147929686  -Training accuracy : 0.8018868\n",
      "epoch:  836  -  cost : 0.50370324  -MSE  8.87943913518643  -Training accuracy : 0.8018868\n",
      "epoch:  837  -  cost : 0.50337416  -MSE  8.880728427406476  -Training accuracy : 0.8018868\n",
      "epoch:  838  -  cost : 0.5030446  -MSE  8.882015119433216  -Training accuracy : 0.8018868\n",
      "epoch:  839  -  cost : 0.50271446  -MSE  8.883297035813543  -Training accuracy : 0.8018868\n",
      "epoch:  840  -  cost : 0.5023838  -MSE  8.884573874528146  -Training accuracy : 0.803066\n",
      "epoch:  841  -  cost : 0.5020524  -MSE  8.885841905440525  -Training accuracy : 0.803066\n",
      "epoch:  842  -  cost : 0.5017205  -MSE  8.887110376086596  -Training accuracy : 0.803066\n",
      "epoch:  843  -  cost : 0.5013881  -MSE  8.888377252688437  -Training accuracy : 0.8042453\n",
      "epoch:  844  -  cost : 0.5010552  -MSE  8.889639356494262  -Training accuracy : 0.8042453\n",
      "epoch:  845  -  cost : 0.50072163  -MSE  8.890896291560464  -Training accuracy : 0.8042453\n",
      "epoch:  846  -  cost : 0.50038755  -MSE  8.892150199058534  -Training accuracy : 0.807783\n",
      "epoch:  847  -  cost : 0.5000529  -MSE  8.893398355299723  -Training accuracy : 0.807783\n",
      "epoch:  848  -  cost : 0.49971777  -MSE  8.89464601101781  -Training accuracy : 0.807783\n",
      "epoch:  849  -  cost : 0.49938208  -MSE  8.89589024769484  -Training accuracy : 0.8089623\n",
      "epoch:  850  -  cost : 0.49904582  -MSE  8.897131332787643  -Training accuracy : 0.8089623\n",
      "epoch:  851  -  cost : 0.49870905  -MSE  8.898366177068628  -Training accuracy : 0.8089623\n",
      "epoch:  852  -  cost : 0.4983717  -MSE  8.899598063761438  -Training accuracy : 0.8089623\n",
      "epoch:  853  -  cost : 0.4980335  -MSE  8.900823866754093  -Training accuracy : 0.8089623\n",
      "epoch:  854  -  cost : 0.49769485  -MSE  8.902047106569178  -Training accuracy : 0.8101415\n",
      "epoch:  855  -  cost : 0.49735555  -MSE  8.903266849133843  -Training accuracy : 0.8113208\n",
      "epoch:  856  -  cost : 0.4970157  -MSE  8.904485732715417  -Training accuracy : 0.8113208\n",
      "epoch:  857  -  cost : 0.49667534  -MSE  8.905702613319262  -Training accuracy : 0.8125\n",
      "epoch:  858  -  cost : 0.49633437  -MSE  8.90692052159254  -Training accuracy : 0.8125\n",
      "epoch:  859  -  cost : 0.49599296  -MSE  8.908135101224278  -Training accuracy : 0.8136792\n",
      "epoch:  860  -  cost : 0.49565092  -MSE  8.909348265027914  -Training accuracy : 0.8160377\n",
      "epoch:  861  -  cost : 0.49530843  -MSE  8.910558623529719  -Training accuracy : 0.8160377\n",
      "epoch:  862  -  cost : 0.49496529  -MSE  8.911764113874355  -Training accuracy : 0.8160377\n",
      "epoch:  863  -  cost : 0.49462163  -MSE  8.9129673806353  -Training accuracy : 0.817217\n",
      "epoch:  864  -  cost : 0.4942773  -MSE  8.914166710364379  -Training accuracy : 0.8183962\n",
      "epoch:  865  -  cost : 0.4939322  -MSE  8.915361972750919  -Training accuracy : 0.8183962\n",
      "epoch:  866  -  cost : 0.49358648  -MSE  8.91655379108526  -Training accuracy : 0.8183962\n",
      "epoch:  867  -  cost : 0.49324015  -MSE  8.917744555517219  -Training accuracy : 0.8195755\n",
      "epoch:  868  -  cost : 0.49289337  -MSE  8.918933823446219  -Training accuracy : 0.8195755\n",
      "epoch:  869  -  cost : 0.49254638  -MSE  8.920123624575643  -Training accuracy : 0.8195755\n",
      "epoch:  870  -  cost : 0.49219874  -MSE  8.921347503804103  -Training accuracy : 0.8195755\n",
      "epoch:  871  -  cost : 0.49185055  -MSE  8.922562609968534  -Training accuracy : 0.8207547\n",
      "epoch:  872  -  cost : 0.49150193  -MSE  8.923766095318355  -Training accuracy : 0.8207547\n",
      "epoch:  873  -  cost : 0.49115267  -MSE  8.924965117917377  -Training accuracy : 0.8207547\n",
      "epoch:  874  -  cost : 0.49080294  -MSE  8.92615687117378  -Training accuracy : 0.8207547\n",
      "epoch:  875  -  cost : 0.49045268  -MSE  8.92734166639975  -Training accuracy : 0.8207547\n",
      "epoch:  876  -  cost : 0.49010175  -MSE  8.928519886692888  -Training accuracy : 0.8207547\n",
      "epoch:  877  -  cost : 0.48975033  -MSE  8.929692969680138  -Training accuracy : 0.8207547\n",
      "epoch:  878  -  cost : 0.4893983  -MSE  8.93085984643047  -Training accuracy : 0.8207547\n",
      "epoch:  879  -  cost : 0.48904574  -MSE  8.932021488892403  -Training accuracy : 0.821934\n",
      "epoch:  880  -  cost : 0.4886926  -MSE  8.933180669260967  -Training accuracy : 0.821934\n",
      "epoch:  881  -  cost : 0.4883389  -MSE  8.93433408298326  -Training accuracy : 0.8231132\n",
      "epoch:  882  -  cost : 0.48798472  -MSE  8.935476459314106  -Training accuracy : 0.8231132\n",
      "epoch:  883  -  cost : 0.48762992  -MSE  8.93661393616674  -Training accuracy : 0.8231132\n",
      "epoch:  884  -  cost : 0.48727453  -MSE  8.937747615865224  -Training accuracy : 0.8231132\n",
      "epoch:  885  -  cost : 0.4869186  -MSE  8.938877921701852  -Training accuracy : 0.8231132\n",
      "epoch:  886  -  cost : 0.48656204  -MSE  8.940005528971398  -Training accuracy : 0.8231132\n",
      "epoch:  887  -  cost : 0.48620504  -MSE  8.941129666635021  -Training accuracy : 0.8231132\n",
      "epoch:  888  -  cost : 0.48584732  -MSE  8.942248817092668  -Training accuracy : 0.8242925\n",
      "epoch:  889  -  cost : 0.48548907  -MSE  8.943364913146539  -Training accuracy : 0.8242925\n",
      "epoch:  890  -  cost : 0.48513025  -MSE  8.944477744101361  -Training accuracy : 0.8242925\n",
      "epoch:  891  -  cost : 0.48477086  -MSE  8.945587145960491  -Training accuracy : 0.8242925\n",
      "epoch:  892  -  cost : 0.4844112  -MSE  8.946725030300128  -Training accuracy : 0.8242925\n",
      "epoch:  893  -  cost : 0.48405096  -MSE  8.94785699613078  -Training accuracy : 0.8266509\n",
      "epoch:  894  -  cost : 0.48369008  -MSE  8.948983493573307  -Training accuracy : 0.8278302\n",
      "epoch:  895  -  cost : 0.4833286  -MSE  8.95010717568049  -Training accuracy : 0.8278302\n",
      "epoch:  896  -  cost : 0.48296672  -MSE  8.951227790627522  -Training accuracy : 0.8278302\n",
      "epoch:  897  -  cost : 0.48260418  -MSE  8.952341427068836  -Training accuracy : 0.8278302\n",
      "epoch:  898  -  cost : 0.482241  -MSE  8.953451115791202  -Training accuracy : 0.8278302\n",
      "epoch:  899  -  cost : 0.48187718  -MSE  8.954558216107708  -Training accuracy : 0.8278302\n",
      "epoch:  900  -  cost : 0.4815128  -MSE  8.955659770608534  -Training accuracy : 0.8278302\n",
      "epoch:  901  -  cost : 0.4811479  -MSE  8.95675660828427  -Training accuracy : 0.8278302\n",
      "epoch:  902  -  cost : 0.4807823  -MSE  8.957850332411219  -Training accuracy : 0.8278302\n",
      "epoch:  903  -  cost : 0.48041627  -MSE  8.95894051563247  -Training accuracy : 0.8278302\n",
      "epoch:  904  -  cost : 0.48004964  -MSE  8.960026827122158  -Training accuracy : 0.8278302\n",
      "epoch:  905  -  cost : 0.47968236  -MSE  8.961112162259216  -Training accuracy : 0.8278302\n",
      "epoch:  906  -  cost : 0.4793145  -MSE  8.962192949359585  -Training accuracy : 0.8278302\n",
      "epoch:  907  -  cost : 0.47894603  -MSE  8.963268836221612  -Training accuracy : 0.8278302\n",
      "epoch:  908  -  cost : 0.47857702  -MSE  8.964340614010958  -Training accuracy : 0.8278302\n",
      "epoch:  909  -  cost : 0.47820744  -MSE  8.965408256423213  -Training accuracy : 0.8290094\n",
      "epoch:  910  -  cost : 0.4778373  -MSE  8.966471951345232  -Training accuracy : 0.8290094\n",
      "epoch:  911  -  cost : 0.47746637  -MSE  8.967531712153365  -Training accuracy : 0.8290094\n",
      "epoch:  912  -  cost : 0.47709504  -MSE  8.968588050652643  -Training accuracy : 0.8290094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  913  -  cost : 0.47672307  -MSE  8.969639617985067  -Training accuracy : 0.8290094\n",
      "epoch:  914  -  cost : 0.4763505  -MSE  8.970689378073518  -Training accuracy : 0.8290094\n",
      "epoch:  915  -  cost : 0.47597733  -MSE  8.97173546549029  -Training accuracy : 0.8290094\n",
      "epoch:  916  -  cost : 0.47560358  -MSE  8.972776678901445  -Training accuracy : 0.8290094\n",
      "epoch:  917  -  cost : 0.4752292  -MSE  8.973815584327532  -Training accuracy : 0.8290094\n",
      "epoch:  918  -  cost : 0.47485423  -MSE  8.974851244080288  -Training accuracy : 0.8301887\n",
      "epoch:  919  -  cost : 0.47447866  -MSE  8.975884771174638  -Training accuracy : 0.8301887\n",
      "epoch:  920  -  cost : 0.4741026  -MSE  8.976914082284415  -Training accuracy : 0.8301887\n",
      "epoch:  921  -  cost : 0.47372574  -MSE  8.9779451836507  -Training accuracy : 0.8325472\n",
      "epoch:  922  -  cost : 0.4733483  -MSE  8.978968792584439  -Training accuracy : 0.8337264\n",
      "epoch:  923  -  cost : 0.47297022  -MSE  8.979989317471983  -Training accuracy : 0.8349057\n",
      "epoch:  924  -  cost : 0.4725916  -MSE  8.981007965529663  -Training accuracy : 0.8349057\n",
      "epoch:  925  -  cost : 0.47221237  -MSE  8.982025812542867  -Training accuracy : 0.8349057\n",
      "epoch:  926  -  cost : 0.47183245  -MSE  8.983042036326884  -Training accuracy : 0.8349057\n",
      "epoch:  927  -  cost : 0.47145197  -MSE  8.984054393821053  -Training accuracy : 0.8360849\n",
      "epoch:  928  -  cost : 0.47107086  -MSE  8.985066289392945  -Training accuracy : 0.8372642\n",
      "epoch:  929  -  cost : 0.47068924  -MSE  8.98607534145706  -Training accuracy : 0.8372642\n",
      "epoch:  930  -  cost : 0.4703069  -MSE  8.98708153028895  -Training accuracy : 0.8372642\n",
      "epoch:  931  -  cost : 0.469924  -MSE  8.988085103765956  -Training accuracy : 0.8372642\n",
      "epoch:  932  -  cost : 0.46954045  -MSE  8.989086816373389  -Training accuracy : 0.8372642\n",
      "epoch:  933  -  cost : 0.46915632  -MSE  8.99008648921591  -Training accuracy : 0.8372642\n",
      "epoch:  934  -  cost : 0.46877167  -MSE  8.991084657148063  -Training accuracy : 0.8372642\n",
      "epoch:  935  -  cost : 0.46838632  -MSE  8.99208057294654  -Training accuracy : 0.8372642\n",
      "epoch:  936  -  cost : 0.46800038  -MSE  8.993073505938948  -Training accuracy : 0.8372642\n",
      "epoch:  937  -  cost : 0.46761373  -MSE  8.99406499354513  -Training accuracy : 0.8372642\n",
      "epoch:  938  -  cost : 0.46722665  -MSE  8.995054237368864  -Training accuracy : 0.8384434\n",
      "epoch:  939  -  cost : 0.46683905  -MSE  8.996041665469505  -Training accuracy : 0.8396226\n",
      "epoch:  940  -  cost : 0.4664509  -MSE  8.997036780363587  -Training accuracy : 0.8396226\n",
      "epoch:  941  -  cost : 0.46606222  -MSE  8.998028602242481  -Training accuracy : 0.8396226\n",
      "epoch:  942  -  cost : 0.46567288  -MSE  8.999019148419093  -Training accuracy : 0.8396226\n",
      "epoch:  943  -  cost : 0.46528295  -MSE  9.000011685733375  -Training accuracy : 0.8419811\n",
      "epoch:  944  -  cost : 0.46489245  -MSE  9.00100249505166  -Training accuracy : 0.8419811\n",
      "epoch:  945  -  cost : 0.4645013  -MSE  9.001988170371634  -Training accuracy : 0.8419811\n",
      "epoch:  946  -  cost : 0.4641094  -MSE  9.00297176703776  -Training accuracy : 0.8419811\n",
      "epoch:  947  -  cost : 0.4637167  -MSE  9.003949377567036  -Training accuracy : 0.8419811\n",
      "epoch:  948  -  cost : 0.46332347  -MSE  9.00492455854567  -Training accuracy : 0.8419811\n",
      "epoch:  949  -  cost : 0.4629296  -MSE  9.005898283728586  -Training accuracy : 0.8419811\n",
      "epoch:  950  -  cost : 0.46253505  -MSE  9.006870614582144  -Training accuracy : 0.8419811\n",
      "epoch:  951  -  cost : 0.46213996  -MSE  9.007841919273437  -Training accuracy : 0.8419811\n",
      "epoch:  952  -  cost : 0.4617442  -MSE  9.008812255361786  -Training accuracy : 0.8431604\n",
      "epoch:  953  -  cost : 0.4613478  -MSE  9.009780105865705  -Training accuracy : 0.8431604\n",
      "epoch:  954  -  cost : 0.46095082  -MSE  9.010746736899927  -Training accuracy : 0.8431604\n",
      "epoch:  955  -  cost : 0.4605532  -MSE  9.011711061642153  -Training accuracy : 0.8431604\n",
      "epoch:  956  -  cost : 0.460155  -MSE  9.012675734055835  -Training accuracy : 0.8431604\n",
      "epoch:  957  -  cost : 0.45975614  -MSE  9.013636765934722  -Training accuracy : 0.8431604\n",
      "epoch:  958  -  cost : 0.45935664  -MSE  9.014596078502864  -Training accuracy : 0.8431604\n",
      "epoch:  959  -  cost : 0.4589566  -MSE  9.015552069821139  -Training accuracy : 0.8431604\n",
      "epoch:  960  -  cost : 0.45855585  -MSE  9.016507113152656  -Training accuracy : 0.8431604\n",
      "epoch:  961  -  cost : 0.45815438  -MSE  9.01746128649643  -Training accuracy : 0.8431604\n",
      "epoch:  962  -  cost : 0.45775202  -MSE  9.018410574955867  -Training accuracy : 0.8431604\n",
      "epoch:  963  -  cost : 0.4573491  -MSE  9.019359879578296  -Training accuracy : 0.8443396\n",
      "epoch:  964  -  cost : 0.4569456  -MSE  9.02030943783677  -Training accuracy : 0.8443396\n",
      "epoch:  965  -  cost : 0.4565414  -MSE  9.021258344404664  -Training accuracy : 0.8443396\n",
      "epoch:  966  -  cost : 0.45613658  -MSE  9.022206262829176  -Training accuracy : 0.8443396\n",
      "epoch:  967  -  cost : 0.45573115  -MSE  9.023152313381425  -Training accuracy : 0.8455189\n",
      "epoch:  968  -  cost : 0.45532507  -MSE  9.024098009491826  -Training accuracy : 0.8455189\n",
      "epoch:  969  -  cost : 0.45491847  -MSE  9.025043322345917  -Training accuracy : 0.8455189\n",
      "epoch:  970  -  cost : 0.45451123  -MSE  9.025985881197494  -Training accuracy : 0.8466981\n",
      "epoch:  971  -  cost : 0.45410335  -MSE  9.026929014920277  -Training accuracy : 0.8466981\n",
      "epoch:  972  -  cost : 0.45369482  -MSE  9.027869438145187  -Training accuracy : 0.8466981\n",
      "epoch:  973  -  cost : 0.45328563  -MSE  9.02880642850187  -Training accuracy : 0.8478774\n",
      "epoch:  974  -  cost : 0.45287603  -MSE  9.029742722139876  -Training accuracy : 0.8478774\n",
      "epoch:  975  -  cost : 0.45246598  -MSE  9.030707297406698  -Training accuracy : 0.8478774\n",
      "epoch:  976  -  cost : 0.45205536  -MSE  9.03166789953182  -Training accuracy : 0.8478774\n",
      "epoch:  977  -  cost : 0.45164406  -MSE  9.032625150200124  -Training accuracy : 0.8478774\n",
      "epoch:  978  -  cost : 0.4512321  -MSE  9.033577960630746  -Training accuracy : 0.8478774\n",
      "epoch:  979  -  cost : 0.4508195  -MSE  9.034526433069551  -Training accuracy : 0.8478774\n",
      "epoch:  980  -  cost : 0.4504062  -MSE  9.03547058141581  -Training accuracy : 0.8478774\n",
      "epoch:  981  -  cost : 0.44999233  -MSE  9.036413096107712  -Training accuracy : 0.8478774\n",
      "epoch:  982  -  cost : 0.44957784  -MSE  9.03735231710568  -Training accuracy : 0.8502358\n",
      "epoch:  983  -  cost : 0.4491627  -MSE  9.038289171440187  -Training accuracy : 0.8502358\n",
      "epoch:  984  -  cost : 0.44874695  -MSE  9.03922232876368  -Training accuracy : 0.8502358\n",
      "epoch:  985  -  cost : 0.4483305  -MSE  9.04015396307865  -Training accuracy : 0.8502358\n",
      "epoch:  986  -  cost : 0.44791347  -MSE  9.041083994253338  -Training accuracy : 0.8525943\n",
      "epoch:  987  -  cost : 0.4474958  -MSE  9.042010953071465  -Training accuracy : 0.8525943\n",
      "epoch:  988  -  cost : 0.44707748  -MSE  9.042935293259168  -Training accuracy : 0.8525943\n",
      "epoch:  989  -  cost : 0.44665837  -MSE  9.043851543076318  -Training accuracy : 0.8525943\n",
      "epoch:  990  -  cost : 0.4462385  -MSE  9.044766517044332  -Training accuracy : 0.8525943\n",
      "epoch:  991  -  cost : 0.44581813  -MSE  9.04568198213381  -Training accuracy : 0.8525943\n",
      "epoch:  992  -  cost : 0.44539708  -MSE  9.04660334258948  -Training accuracy : 0.8537736\n",
      "epoch:  993  -  cost : 0.44497526  -MSE  9.047520573099453  -Training accuracy : 0.8537736\n",
      "epoch:  994  -  cost : 0.44455296  -MSE  9.04844736070986  -Training accuracy : 0.8537736\n",
      "epoch:  995  -  cost : 0.44413015  -MSE  9.049376858881324  -Training accuracy : 0.8537736\n",
      "epoch:  996  -  cost : 0.44370666  -MSE  9.050302834107116  -Training accuracy : 0.8537736\n",
      "epoch:  997  -  cost : 0.44328257  -MSE  9.051229207303075  -Training accuracy : 0.8549528\n",
      "epoch:  998  -  cost : 0.44285786  -MSE  9.052156109817338  -Training accuracy : 0.8549528\n",
      "epoch:  999  -  cost : 0.44243255  -MSE  9.053081162513994  -Training accuracy : 0.8549528\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b8dd3e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_history=[]\n",
    "accuracy_history=[]\n",
    "for epoch in range(epochs):\n",
    "    sess.run(training_step,feed_dict={x:train_x,y_:train_y})\n",
    "    cost=sess.run(cost_function,feed_dict={x:train_x,y_:train_y})\n",
    "    cost_history=np.append(cost_history,cost)\n",
    "    correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "    pred_y=sess.run(y,feed_dict={x:test_x})\n",
    "    mse=tf.reduce_mean(tf.square(pred_y - test_y))\n",
    "    mse_=sess.run(mse)\n",
    "    mse_history.append(mse_)\n",
    "    accuracy=(sess.run(accuracy,feed_dict={x:train_x,y_:train_y}))\n",
    "    accuracy_history.append(accuracy)\n",
    "    print('epoch: ',epoch,\" - \",\"cost :\", cost,\" -MSE \",mse_,\" -Training accuracy :\",accuracy)\n",
    "    \n",
    "plt.plot(accuracy_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8443396\n",
      "MSE: 9.0531\n"
     ]
    }
   ],
   "source": [
    "#Print the final accuracy\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(\"Test Accuracy: \",(sess.run(accuracy, feed_dict={x:test_x,y_:test_y})))\n",
    "\n",
    "pred_y=sess.run(y, feed_dict={x:test_x})\n",
    "mse=tf.reduce_mean(tf.square(pred_y-test_y))\n",
    "print(\"MSE: %.4f\" %sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
